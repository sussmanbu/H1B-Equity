---
title: "blogpost5"
description:  |
  This blog post will build on blogpost4 to merge the 3rd dataset by case
  numbers
author: "team3"
date: "2024-04-10"
date-modified: "2024-04-10"
draft: FALSE
---

```{r}
library(readxl)
library(readr)
library(dplyr)
```

```{r}
df <- readRDS("dataset/cleaned_disclosure.rds")
print(df)
```
# Combining Dataset

We selected this dataset, updated in 2024, because it comprehensively represents the evolution of U.S. immigration policy from the Trump to the Biden administration. It provides a wealth of columns suitable for statistical modeling. To enrich our analysis, we plan to merge it with two other datasets. The first includes information on the applicants' universities and majors, while the second contains data on their incomes, allowing us to construct a linear regression model. Once merged, we will have a dataframe with approximately 30+ columns.

This is the line of code of how to read xlsx files and convert into rds. We commented out after done
```{r}

# rds for the other two projects
# df2 <- read_excel("dataset/worksite.xlsx")
# saveRDS(df2, "dataset/LCA_worksite.rds")

# df3 <- read_excel("dataset/appendix.xlsx")
# saveRDS(df3, "dataset/LCA_appendix.rds")
```

```{r}
# read in df2

df2 <- readRDS("dataset/LCA_worksite.rds")

print(df2)
```

```{r}
# read in df3

df3 <- readRDS("dataset/appendix.rds")

print(df3)
```

## Merging Process
To merge the dataset together, we can find matching case numbers.

```{r}
# merge df1 and df2 by case number
# merge_1 <- merge(df, df2, by="CASE_NUMBER", all = TRUE)
```

```{r}
# final_merge <- merge(merge_1, df3, by="CASE_NUMBER", all = TRUE)
```

```{r}
# visualize our new dataset
# final_merge
```

```{r}
# save as new rds for final_merge
# saveRDS(final_merge, "dataset/final_merge.rds")
```

# Start HERE

Just load in the final_merge dataset
```{r}
final_df <- readRDS("dataset/final_merge.rds")
print(final_df)
```

```{r}
colnames(final_df)
```
# Initial Thoughts
After having complete columns, here are some thoughts we have in mind which guide our exploration data analysis

### Employment Trends
What are some most common jobs titles, industries, or the duration of employment contracts.

```{r}
print(sort(table(final_df$JOB_TITLE), decreasing=TRUE)[1:5])
```

```{r}
filtered_data <- final_df |>
  filter(FULL_TIME_POSITION == "Y") |>
  count(JOB_TITLE, sort = TRUE)

print(filtered_data)
```
From this result, we can see that tech related jobs are still the highest percentage of getting visas.

### Geographical Patterns
In blog-post-4, we learned that New York and Massachusetts have a negative likelihood of getting fulltime position, perhaps we can investigate more here.

### Build on Logistic Regression
This time with more features/columns, we can build a better logistic regression model to predict approved or denial


### Linear Regression
This time with numerical columns(wage), we can predict wages based on various features








# data exploration 
case status 
```{r}
library(dplyr)
status_counts <- df %>% count(CASE_STATUS)
ggplot(status_counts, aes(x = "", y = n, fill = CASE_STATUS)) +
    geom_bar(width = 1, stat = "identity") +
    coord_polar(theta = "y") +
    theme_void() +
    labs(fill = "Case Status", title = "Distribution of CASE_STATUS")
```
## import raw 
```{r}

mydata=PERM_Disclosure_Data_FY2018_EOY
# Set seed for reproducibility
set.seed(123)

# Sample 1% of the data
mydata <- mydata[sample(nrow(mydata), size = 0.01 * nrow(mydata)), ]

status_counts <- mydata %>% count(CASE_STATUS)
ggplot(status_counts, aes(x = "", y = n, fill = CASE_STATUS)) +
    geom_bar(width = 1, stat = "identity") +
    coord_polar(theta = "y") +
    theme_void() +
    labs(fill = "Case Status", title = "Distribution of CASE_STATUS")
```
```{r}

lm_result <- lm(data_dif ~ PW_AMOUNT_9089, data = mydata)
summary(lm_result)

```





