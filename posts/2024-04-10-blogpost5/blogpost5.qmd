---
title: "blogpost5"
description:  |
  This blog post will build on blogpost4 to merge the 3rd dataset by case
  numbers
author: "team3"
date: "2024-04-10"
date-modified: "2024-04-10"
draft: FALSE
---

```{r}
library(readxl)
library(readr)
library(dplyr)
library(tidymodels)
library(cluster)
library(ggplot2)
library(lubridate)
```

```{r}
df <- readRDS("dataset/cleaned_disclosure.rds")
head(df)
```
# Combining Dataset

We selected this dataset, updated in 2024, because it comprehensively represents the evolution of U.S. immigration policy from the Trump to the Biden administration. It provides a wealth of columns suitable for statistical modeling. To enrich our analysis, we plan to merge it with two other datasets. The first includes information on the applicants' universities and majors, while the second contains data on their incomes, allowing us to construct a linear regression model. Once merged, we will have a dataframe with approximately 30+ columns.

This is the line of code of how to read xlsx files and convert into rds. We commented out after done
```{r}
# rds for the other two projects
# df2 <- read_excel("dataset/worksite.xlsx")
# saveRDS(df2, "dataset/LCA_worksite.rds")

# df3 <- read_excel("dataset/appendix.xlsx")
# saveRDS(df3, "dataset/LCA_appendix.rds")
```

```{r}
# read in df2

df2 <- readRDS("dataset/LCA_worksite.rds")

```

```{r}
# read in df3

df3 <- readRDS("dataset/appendix.rds")

```

## Merging Process
To merge the dataset together, we can find matching case numbers.

```{r}
# merge df1 and df2 by case number
# merge_1 <- merge(df, df2, by="CASE_NUMBER", all = TRUE)
```

```{r}
# final_merge <- merge(merge_1, df3, by="CASE_NUMBER", all = TRUE)
```

```{r}
# visualize our new dataset
# final_merge
```

```{r}
# save as new rds for final_merge
# saveRDS(final_merge, "dataset/final_merge.rds")
```

# Data Background
This is the dataset we are combining..

# Start HERE - Import Data
```{r}
# just load in this dataset
final_df <- readRDS("dataset/final_merge.rds")
head(final_df)
```
# Exploration Data Analysis


```{r}
# extract year from date
final_df$YEAR_RECEIVED = as.integer(substr(final_df$RECEIVED_DATE, 1, 4))
final_df$MONTH_RECEIVED = month(final_df$RECEIVED_DATE)
final_df$DAY_RECEIVED = day(final_df$RECEIVED_DATE)
```
Let's do the same for the remainder date columns

```{r}

final_df$YEAR_END = as.integer(substr(final_df$END_DATE, 1, 4))
final_df$MONTH_END = month(final_df$END_DATE)
final_df$DAY_END = day(final_df$END_DATE)

final_df$YEAR_DECISION = as.integer(substr(final_df$DECISION_DATE, 1, 4))
final_df$MONTH_DECISION = month(final_df$DECISION_DATE)
final_df$DAY_DECISION = day(final_df$DECISION_DATE)


final_df$YEAR_BEGIN = as.integer(substr(final_df$BEGIN_DATE, 1, 4))
final_df$MONTH_BEGIN = month(final_df$BEGIN_DATE)
final_df$DAY_BEGIN = day(final_df$BEGIN_DATE)
```

```{r}
head(final_df)
```

```{r}
colnames(final_df)
```


```{r}
# let's check missing values
print(colSums(is.na(final_df)))
```

```{r}
unique(final_df["FULL_TIME_POSITION"])
```

### Employment Trends
What are some most common jobs titles, industries, or the duration of employment contracts.

```{r}
# filter the dataset which jobs have most frequency and applicant get full time position
filtered_data <- final_df |>
  filter(FULL_TIME_POSITION == "Y") |>
  count(JOB_TITLE, sort = TRUE) |>
  top_n(5)

print(filtered_data)
```
This is the most frequent job titles appear in the dataset. From this result, we can see that tech related jobs are still the highest percentage of strong hire in United States.


```{r}
# filter the dataset which jobs have most frequency and applicant get full time position
filtered_data <- final_df |>
  count(EMPLOYER_NAME, sort = TRUE) |>
  top_n(5)

print(filtered_data)
```
Big companies has always mass hire workers since they have more budget. It is no surprise to see Amazon to be at top.


case status 
```{r}
case_df <- final_df %>%
  count(CASE_STATUS) %>%
  mutate(percent = n / sum(n) * 100)

ggplot(case_df, aes(x = CASE_STATUS, y = percent, fill = CASE_STATUS)) +
  geom_col() +
  geom_text(aes(label = sprintf("%.1f%%", percent)), position = position_stack(vjust = 0.5), hjust = 1) +
  labs(fill = "Case Status", title = "Distribution of CASE_STATUS")
```
Looking at the plot, it might not be suitable to build a classifier to predict labels since it is high imbalance.
What does each label mean?

When application is certified, it allows the employer to proceed with next steps of the visa process, which include petitioning U.S. Citizenship and Immigration Services (USCIS) for a visa on behalf of the foreign worker.

We also notice a 9% certified - withdrawn. In other word, this is saying the application is approved but employer chose to withdraw it before foreign worker began employment. This can happen due to several reason.

Company's budget cut

candidate issues - candidate might accepted job offers elsewhere

Next, we will observe fulltime position.
```{r}
case_df <- final_df %>%
  count(FULL_TIME_POSITION) %>%
  mutate(percent = n / sum(n) * 100)

ggplot(case_df, aes(x = FULL_TIME_POSITION, y = percent, fill = FULL_TIME_POSITION)) +
  geom_col() +
  geom_text(aes(label = sprintf("%.1f%%", percent)), position = position_stack(vjust = 0.5), hjust = 1) +
  labs(fill = "FULL_TIME_POSITION", title = "Distribution of FULL_TIME_POSITION")
```
Again, we see that highly imbalance target labels. Remember that employers who are seeking to hire foreign workers must submit various forms of applications. Those candidates who received full time positions from employers are considered strong candidates which is why employers are willing to submit applications to OFLC.



```{r}
# Identify states with high number of applicants or high approval rates
state_df <- final_df

state_df <- na.omit(state_df[c("EMPLOYER_STATE", "CASE_STATUS")])

state_data <- state_df |>
  group_by(EMPLOYER_STATE) |>
  summarise(
    Total_Applications = n(),
    Approved_Applications = sum(CASE_STATUS == "Approved")
  )

```

```{r}
# Sort by total applications
state_data_sorted_applicants <- state_data %>%
  arrange(desc(Total_Applications))

top_states <- state_data_sorted_applicants |>
  top_n(20, Total_Applications)
```


```{r}
# visualizing

# Plot for Total Applications
ggplot(top_states, aes(x = reorder(EMPLOYER_STATE, -Total_Applications), y = Total_Applications)) +
  geom_bar(stat = "identity", fill = "GREY") +
  theme(axis.text.y = element_text(size = 6)) +  # Smaller text size for states
  labs(title = "Total Visa Applications by State", x = "State", y = "Number of Applications") +
  coord_flip()  # Flips the coordinates to make labels readable
```
Top 20 states where received the highest number of applications. Now you know what states to look for jobs.

```{r}

# mydata=PERM_Disclosure_Data_FY2018_EOY
# Set seed for reproducibility
# set.seed(123)

# Sample 1% of the data
# mydata <- mydata[sample(nrow(mydata), size = 0.01 * nrow(mydata)), ]

# status_counts <- mydata %>% count(CASE_STATUS)
# ggplot(status_counts, aes(x = "", y = n, fill = CASE_STATUS)) +
    # geom_bar(width = 1, stat = "identity") +
    # coord_polar(theta = "y") +
    # theme_void() +
    # labs(fill = "Case Status", title = "Distribution of CASE_STATUS")
```

```{r}

# lm_result <- lm(data_dif ~ PW_AMOUNT_9089, data = mydata)
# summary(lm_result)

```

### Clustering Analysis
We can make use of various clustering techniques to group similar applicants together.
```{r}

filtered_df <- final_df %>%
  # filter(JOB_TITLE %in% c("Software Engineer", "Assistant Professor", "Business Analyst", "Machine Learning Engineer"),
  # FULL_TIME_POSITION %in% c("Y", "N")) %>%
  select(WAGE_RATE_OF_PAY_FROM, PREVAILING_WAGE)

head(filtered_df)
```


```{r}
# create recipe
rec <- recipe(~ ., data=filtered_df) |>
  step_dummy(all_nominal()) |>
  prep()

cluster_data <- bake(rec, new_data=NULL)

# perform clustering
set.seed(42)
k <- 5
clustered_result <- kmeans(cluster_data, centers=k)

# visualize cluster
filtered_df$cluster <- factor(clustered_result$cluster)
head(filtered_df)
```

```{r}
ggplot(filtered_df, aes(x = WAGE_RATE_OF_PAY_FROM, y = PREVAILING_WAGE)) +
  geom_point(aes(color = cluster)) +
  # facet_wrap(~ cluster) +
  theme_minimal() +
  labs(title = "Wage Rate vs. Prevailing Wage by Cluster") +
  scale_x_log10() +
  scale_y_log10()
  
```

# Observation and Limitation

WAGE_RATE_OF_PAY_FROM is the minimum wage pay at the worksite, 

PREVAILING_WAGE is the average wage paid to similar employed workers in the field

Looking at the result of KMeans with only two numerical columns, it was not necessary to make use of Kmeans because is obvious to notice a linear relationship. There's a positive correlation between these two features. It is expected that employers are pay more at the worksite than prevailng wages.

For example, applicants who are highly specialized / high demand positions mostly lie in cluster 4. 

To recall, one limitation of this KMeans is the choice of cluster, k, and limited numerical columns to work with. 


```{r}
yearly_wage <- final_df %>%
  group_by(YEAR_BEGIN) %>%
  summarise(Avg_Prevailing_Wage = mean(PREVAILING_WAGE, na.rm = TRUE))

ggplot(yearly_wage, aes(x = YEAR_BEGIN, y = Avg_Prevailing_Wage)) +
  geom_line() +  # Use a line plot to show trends over time
  geom_point() +  # Add points to highlight each year's average wage
  labs(title = "Average Prevailing Wage Per Year",
       x = "Year",
       y = "Average Prevailing Wage") 

```
we see a sharp drop in year 2021 due to COVID-19. 


# What's Next

### Build on Logistic Regression
This time with more features/columns, we can build a better logistic regression model to predict approved or denial

### Linear Regression on income
