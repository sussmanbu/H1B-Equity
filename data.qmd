---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
editor: 
  markdown: 
    wrap: 72
---

![](images/analysis.png){width="900"} 

### Datasets Overview

We explore three datasets:

1.  **Travel Data**: Visas per country - from travel.state.gov (U.S Department of State, Department of Consular Affairs)
2.  **Labor Data**: H-1B worker conditions - from dol.gov (U.S Department of Labor)
3.  **Economic Data**: GDP per capita and Unemployment rate per country - from imf.org (Internation Monetary Fund)

### Dataset 1: Travel Data

**Source**

Our first dataset is downloaded from the [U.S. Department of
State](https://travel.state.gov/content/dam/visas/Statistics/Non-Immigrant-Statistics/NIVDetailTables/FYs97-22_NIVDetailTable.xlsx).and
stored in the `dataset` folder. The data was collected by U.S Department
of State, more specifically the Bureau of Consular Affairs.

**Purpose**

The Report of the Visa Office is an annual report providing statistical
information on immigrant and non-immigrant visa issuance by consular
offices, including information visa allocation in numerically limited
categories. The main part of this dataset we want to focus on is the
country of origin of visa recipients for H-1B type visas. Years covered
range from Fiscal Year 1997 to F.Y. 2022 inclusively.

**Description**: The original dataset comes with the following
features/columns:

-   **Country**: Origin country of visa recipients.
-   **H-1B**: H-1B approval count.
-   **FY97 - FY22**: Years 1997 to 2022.

Essentially, this dataset contains information of H-1B approval from year
1997 till 2022 and its originate country.

**Cleaning Process**:

1.  **Read Excel Files**.
2.  **Filter and Group**.
3.  **Create Columns for Each Year**.

#### Cleaning Script

Find the full cleaning script [here](/scripts/load_and_clean_data.R).

**Cleaned Data Example**:

Essentially, the dataset is a large excel file that contains multiple
sheets, processes the data from the 'FY22' sheet to assign continents to
countries, and then aggregates H-1B visa statistics from each sheet by
country, storing the results in a data frame called 'processed_data'.
Next we save the cleaned dataset as **cleaned_h1b_data.rds**. More
details of the code in load_and_clean_data.R

```{r}
library(readr)
#filename <- file.choose("cleaned_h1b_data.rds")
#print(filename)
h1b_country <- 
  read_rds('dataset/cleaned_h1b_data.rds')
```

```{r, results='hide'}
#changing the year to numerical values
current_names <- names(h1b_country)
print(current_names)
#names(df) <- new_names

```

The code below is to rename the columns to understandable years format.

```{r, results='hide'}
colnames(h1b_country) <- c(
  'Country', 'Continent', 
  '1997', '1998', '1999', '2000', '2001', '2002', 
  '2003', '2004', '2005', '2006', '2007', '2008', 
  '2009', '2010', '2011', '2012', '2013', '2014', 
  '2015', '2016', '2017', '2018', '2019', '2020', 
  '2021', '2022'
) 

head(h1b_country)
```

**Reshaped Data**:

The following code shortens the dimension of our dataframe to 3 columns
specially the years columns. Having many years column can make the
dataset hard to analysis or explore.

```{r, results='hide', warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)

h1b_continent <- h1b_country |>
  group_by(Continent)|>
  select(-Country)|>
  summarize(across(everything(), sum, na.rm = TRUE))

data_long <- pivot_longer(h1b_continent, cols = -Continent, 
                          names_to = "year", values_to = "count_h1b")

head(data_long)
```

------------------------------------------------------------------------

### Dataset 2: Labor Data

**Source**: [U.S. Department of
Labor](https://www.dol.gov/agencies/eta/foreign-labor/performance). We
downloaded LCA Programs H-1B and PERM disclosure data in .excel file or
tabular data.

PERM_disclosure_data: A system used by the U.S Department of Labor to
process labor certifications, which is the first step required in the
process for certain foreign nationals to obtain an employment-based
immigrant visa, or green card.

LCA_Program_data: LCA stands for Labor Condition Application. This
application is a component of the H-1B visa process, where employers in
the U.S. must attest to the Department of Labor that they will pay the
H-1B visa holder a wage that is no less than the wage paid to similarly
qualified U.S. workers. It also ensures that employing a foreign worker
will not adversely affect the working conditions of U.S. workers
similarly employed.

**Purpose**

The data is made available as part of a broader government initiative
toward transparency and open governance. This initiative aims to make
government operations more accessible and understandable to the public.
By providing access to historical and current data about
immigration-related labor applications, the dataset aids in understand
trends, evaluate the effectiveness of current policies, and plan future
actions based on solid data. Overall, this dataset is a critical
resource for enhancing understanding and facilitating a data-driven
approach to managing and legislating foreign labor and
immigration-related activities.

**How it was collected?**

Recall that the dataset was provided by OFLC(Office of Foreign Labor
Certification). Employers who are seeking to hire foreign workers must
submit various forms of applications for prevailing wage determinations,
labor certifications, and labor attestations. These applications are
required for different visa programs, such as H-1B, H-2A, H-2B, and
permanent residency applications. Our goal is focus on H-1B but not
limited to explore other programs.

**Description**: \## Background Since there exist many columns, we will
list a few important columns.

**CASE_NUMBER**: Unique identifier assigned to each application
submitted for processing to OFLC.

**CASE_STATUS**: decision on application. Once certified, employer can
proceed with next steps of the visa process to get VISAs for foreign
worker

**FULL_TIME_POSITION**: Y for getting fulltime and N for not getting

**TOTAL_WORKER_POSITIONS**: Total number of foreign workers requested by
employers

**EMPLOYER_NAME**: Business name

**PREVAILING_WAGE**: average wage based on labor market

**WAGE_RATE_OF_PAY_FROM**: wage paid at the site

**WORKSITE_CITY**: City of the worksite

**AGENT_REPRESENTING_EMPLOYER**: employer represented by agent or
attorney, Y or N

**RECEIVED_DATE**: Date the application received by OFLC

**Cleaning Process**:

1.  **Load RDS Files**.
2.  **Merge Data**.

#### Cleaning Script

Find the full cleaning script [here](/scripts/load_and_clean_data.R).

The following steps would be describing how we load our dataset and
merge additional columns. Later down the steps, we will merge the world
economic dataset

```{r, results='hide'}
df <- readRDS("dataset/cleaned_disclosure.rds")
head(df)
```

We selected this dataset, updated in 2024, because it comprehensively
represents the evolution of U.S. immigration policy from the Trump to
the Biden administration. It provides a wealth of columns suitable for
statistical modeling. To enrich our analysis, we plan to merge it with
two other datasets. The first includes information on the applicants'
universities and majors, while the second contains data on their
incomes, allowing us to construct a linear regression model. Once
merged, we will have a dataframe with approximately 30+ columns.

This is the line of code of how to read xlsx files and convert into rds.
We commented out after done. Afterward, we can load in rds files and
merge the columns

```{r}
# rds for the other two projects
# df2 <- read_excel("dataset/worksite.xlsx")
# saveRDS(df2, "dataset/LCA_worksite.rds")

# df3 <- read_excel("dataset/appendix.xlsx")
# saveRDS(df3, "dataset/LCA_appendix.rds")
```

```{r, results='hide'}
# read in df2

df2 <- readRDS("dataset/LCA_worksite.rds")
head(df2)
```

```{r, results='hide'}
# read in df3

df3 <- readRDS("dataset/appendix.rds")
head(df3)
```

To merge the dataset together, we can find matching case numbers. The
additional dataset gives additional information about applicants
university, field of major, date of degree etc...

Here we commented it out since the dataset has already been merged.

```{r}
# merge df1 and df2 by case number
# merge_1 <- merge(df, df2, by="CASE_NUMBER", all = TRUE)
```

```{r}
# final_merge <- merge(merge_1, df3, by="CASE_NUMBER", all = TRUE)
```

```{r}
# save as new rds for final_merge
# saveRDS(final_merge, "dataset/final_merge.rds")
```

Load in the dataset here

```{r, results='hide'}
final_df <- readRDS("dataset/final_merge.rds")
head(final_df)
```

We will extract year, month, day from the date column to prepare dataset
for time-series forecasting later.

```{r}
# extract year, month, day from date
library(lubridate)
final_df$YEAR_RECEIVED = as.integer(substr(final_df$RECEIVED_DATE, 1, 4))
final_df$MONTH_RECEIVED = month(final_df$RECEIVED_DATE)
final_df$DAY_RECEIVED = day(final_df$RECEIVED_DATE)
```

```{r}

final_df$YEAR_END = as.integer(substr(final_df$END_DATE, 1, 4))
final_df$MONTH_END = month(final_df$END_DATE)
final_df$DAY_END = day(final_df$END_DATE)

final_df$YEAR_DECISION = as.integer(substr(final_df$DECISION_DATE, 1, 4))
final_df$MONTH_DECISION = month(final_df$DECISION_DATE)
final_df$DAY_DECISION = day(final_df$DECISION_DATE)


final_df$YEAR_BEGIN = as.integer(substr(final_df$BEGIN_DATE, 1, 4))
final_df$MONTH_BEGIN = month(final_df$BEGIN_DATE)
final_df$DAY_BEGIN = day(final_df$BEGIN_DATE)
```

### Dataset 3: Economic Data

**Source**:
[IMF](https://www.imf.org/external/datamapper/NGDPDPC@WEO/OEMDC/ADVEC/WEOWORLD).

**Purpose**

The International Monetary Fund (IMF) data is collected by the IMF
itself. It gathers data from its member countries and other sources to
monitor economic and financial developments globally. The data is
typically collected through various means, including direct reporting
from member countries, surveys, and other sources such as international
organizations and financial institutions.

**Format**

The format of the IMF data can vary depending on the specific dataset,
but it often includes structured data tables, time series data, and
reports. IMF includes Excel spreadsheets, CSV files, databases, and
interactive online tools, to accommodate different user needs and
preferences.

**Cleaning**

To clean and process H-1B visa data, we need to filters the data where
CLASS_OF_ADMISSION is 'H-1B' and CASE_STATUS is 'Certified'.

1.  selects relevant columns specified in relevant_columns

2.  creates dummy variables for the 'FOREIGN_WORKER_EDUCATION' column
    standardizes country names using a function
    standardize_country_names

3.  Applies this function to two data frames, fy22 and fy23,
    concatenates the cleaned data frames into one (h1b_stacked), and
    finally displays the cleaned data frames for FY22 (fy22_cleaned),
    FY23 (fy23_cleaned), and the concatenated data (h1b_stacked).

```{r message = FALSE, warning = FALSE}
# load dataset
library(readxl)
fy22 <- read_excel('dataset-ignore/PERM_Disclosure_Data_FY2022_Q4.xlsx')
fy23 <- read_excel('dataset-ignore/PERM_Disclosure_Data_FY2023_Q4.xlsx')
```

```{r}
# Load IMF data
imf_gdpc_usd_raw <- read_excel('dataset/IMF_data/imf_gdp_per_capita_usd.xls')
imf_unemployment_raw <- read_excel('dataset/IMF_data/imf_unemployment.xls')
names(imf_gdpc_usd_raw)[1] <- "Country"
names(imf_unemployment_raw)[1] <- "Country"
```

### Dependencies Used For Data.qmd

Below is a list of packages that we use for our dataset

```{r}
suppressPackageStartupMessages(library(tidyverse))
library(tidyr)
library(viridis)
library(fastDummies)
library(ggplot2)
library(dplyr)
library(readxl)
library(readr)
library(lubridate)
```
