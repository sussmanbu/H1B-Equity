
---
title: Data
description: We describe the sources of our data and the cleaning process.
toc: true
draft: false
editor: 
  markdown: 
    wrap: 72
---

![](images/analysis.png){width="900"}

```{r}
library(tidyverse)
```

### Datasets Overview

We explore three datasets:

1.  **Travel Data**: From U.S Department of State.
2.  **Labor Data**: From U.S Department of Labor (H1B Worker
    Conditions).
3.  **Economic Data**: Merged with H1B dataset.

### Dataset 1: Travel Data

**Source**

Our first dataset is downloaded from the [U.S. Department of
State](https://travel.state.gov/content/dam/visas/Statistics/Non-Immigrant-Statistics/NIVDetailTables/FYs97-22_NIVDetailTable.xlsx).and
stored in the `dataset` folder. The data was collected by U.S Department
of State, more specifically the Bureau of Consular Affairs.

**Purpose**

The Report of the Visa Office is an annual report providing statistical
information on immigrant and non-immigrant visa issuance by consular
offices, including information visa allocation in numerically limited
categories. The main part of this dataset we want to focus on is the
country of origin of visa recipients for H1B-type visas. Years covered
range from Fiscal Year 1997 to F.Y. 2022 inclusively.

**Description**: The original dataset comes with the following
features/columns:

-   **Country**: Origin country of visa recipients.
-   **H1B**: H1B approval count.
-   **FY97 - FY22**: Years 1997 to 2022.

Essentially, this dataset contains information of H1B approval from year
1997 till 2022 and its originate country.

**Cleaning Process**:

1.  **Read Excel Files**.
2.  **Filter and Group**.
3.  **Create Columns for Each Year**.

#### Cleaning Script

Find the full cleaning script [here](/scripts/load_and_clean_data.R).

**Cleaned Data Example**:

Essentially, the dataset is a large excel file that contains multiple
sheets, processes the data from the 'FY22' sheet to assign continents to
countries, and then aggregates H-1B visa statistics from each sheet by
country, storing the results in a data frame called 'processed_data'.
Next we save the cleaned dataset as **cleaned_h1b_data.rds**. More
details of the code in load_and_clean_data.R

```{r}
h1b_country <- readRDS('dataset/cleaned_h1b_data.rds')
```

The code below is to rename the columns to understandable years format.

```{r, results='hide'}
colnames(h1b_country) <- c(
  'Country', 'Continent', 
  '1997', '1998', '1999', '2000', '2001', '2002', 
  '2003', '2004', '2005', '2006', '2007', '2008', 
  '2009', '2010', '2011', '2012', '2013', '2014', 
  '2015', '2016', '2017', '2018', '2019', '2020', 
  '2021', '2022'
) 
```

**Reshaped Data**:

To shortens the dimension of our dataframe to 3 columns
specially the years columns. We need to group by the continent column and select by country column.
Next, we will summarize the sum of count results across everything.

------------------------------------------------------------------------

### Dataset 2: Labor Data

**Source**: [U.S. Department of
Labor](https://www.dol.gov/agencies/eta/foreign-labor/performance). We
downloaded LCA Programs H-1B and PERM disclosure data in .excel file or
tabular data.

**Source**: Find the source of the dataset: [here](/dataset/cleaned_disclosure.rds)

**Source 2**: Find the 2nd source of the dataset: [here](/dataset/combined_perm_19_23.csv)

PERM_disclosure_data: A system used by the U.S Department of Labor to
process labor certifications, which is the first step required in the
process for certain foreign nationals to obtain an employment-based
immigrant visa, or green card.

LCA_Program_data: LCA stands for Labor Condition Application. This
application is a component of the H1B visa process, where employers in
the U.S. must attest to the Department of Labor that they will pay the
H1B visa holder a wage that is no less than the wage paid to similarly
qualified U.S. workers. It also ensures that employing a foreign worker
will not adversely affect the working conditions of U.S. workers
similarly employed.

**Purpose**

The data is made available as part of a broader government initiative
toward transparency and open governance. This initiative aims to make
government operations more accessible and understandable to the public.
By providing access to historical and current data about
immigration-related labor applications, the dataset aids in understand
trends, evaluate the effectiveness of current policies, and plan future
actions based on solid data. Overall, this dataset is a critical
resource for enhancing understanding and facilitating a data-driven
approach to managing and legislating foreign labor and
immigration-related activities.

**How it was collected?**

Recall that the dataset was provided by OFLC(Office of Foreign Labor
Certification). Employers who are seeking to hire foreign workers must
submit various forms of applications for prevailing wage determinations,
labor certifications, and labor attestations. These applications are
required for different visa programs, such as H-1B, H-2A, H-2B, and
permanent residency applications. Our goal is focus on H-1B but not
limited to explore other programs.

**Description**: \## Background Since there exist many columns, we will
list a few important columns.

**CASE_NUMBER**: Unique identifier assigned to each application
submitted for processing to OFLC.

**CASE_STATUS**: decision on application. Once certified, employer can
proceed with next steps of the visa process to get VISAs for foreign
worker

**FULL_TIME_POSITION**: Y for getting fulltime and N for not getting

**TOTAL_WORKER_POSITIONS**: Total number of foreign workers requested by
employers

**EMPLOYER_NAME**: Business name

**PREVAILING_WAGE**: average wage based on labor market

**WAGE_RATE_OF_PAY_FROM**: wage paid at the site

**WORKSITE_CITY**: City of the worksite

**AGENT_REPRESENTING_EMPLOYER**: employer represented by agent or
attorney, Y or N

**RECEIVED_DATE**: Date the application received by OFLC

**Cleaning Process**:

1.  **Load RDS Files**.
2.  **Merge Data**.

#### Cleaning Script

Find the full cleaning script [here](/scripts/load_and_clean_data.R).

Because we have dataset that are too big to upload to Github, essentially, we load these datasets from our local machine, and this cleaning script is to filter by H-1B and restrict to relevant columns.

`Case Number`

`Case Status`

`Decision Date`

`Employer name`

`Wage offered`

`country of citizenship`

`foreign worker education`

After that, we split the decision date into years and months for further time analysis, so we can analysis time series data. This is done for the perm_data from year 2019 - 2023, written to the dataset `dataset/combined_perm_19_23.csv`.

The following steps would be describing how we load our dataset and
merge additional columns. Later down the steps, we will merge the world
economic dataset

We selected this dataset, updated in 2024, because it comprehensively
represents the evolution of U.S. immigration policy from the Trump to
the Biden administration. It provides a wealth of columns suitable for
statistical modeling. To enrich our analysis, we plan to merge it with
two other datasets. The first includes information on the applicants'
universities and majors, while the second contains data on their
incomes, allowing us to construct a linear regression model. Once
merged, we will have a dataframe with approximately 30+ columns.

We need  to read xlsx files and convert into rds.
We commented out after done. Afterward, we can load in rds files and
merge the columns

```{r, results='hide'}
df2 <- readRDS("dataset/LCA_worksite.rds")
```

```{r, results='hide'}
df3 <- read_csv("dataset/combined_perm_19_23.csv")
```

To merge the dataset together, we can find matching case numbers. The
additional dataset gives additional information about applicants
university, field of major, date of degree etc...

Load in the dataset here

```{r, results='hide'}
final_df <- readRDS("dataset/final_merge.rds")
head(final_df)
```

We will extract year, month, day from the date column to prepare dataset
for time-series forecasting later.

```{r}
# extract year, month, day from date
library(lubridate)
final_df$YEAR_RECEIVED = as.integer(substr(final_df$RECEIVED_DATE, 1, 4))
final_df$MONTH_RECEIVED = month(final_df$RECEIVED_DATE)
final_df$DAY_RECEIVED = day(final_df$RECEIVED_DATE)
```

```{r}

final_df$YEAR_END = as.integer(substr(final_df$END_DATE, 1, 4))
final_df$MONTH_END = month(final_df$END_DATE)
final_df$DAY_END = day(final_df$END_DATE)

final_df$YEAR_DECISION = as.integer(substr(final_df$DECISION_DATE, 1, 4))
final_df$MONTH_DECISION = month(final_df$DECISION_DATE)
final_df$DAY_DECISION = day(final_df$DECISION_DATE)


final_df$YEAR_BEGIN = as.integer(substr(final_df$BEGIN_DATE, 1, 4))
final_df$MONTH_BEGIN = month(final_df$BEGIN_DATE)
final_df$DAY_BEGIN = day(final_df$BEGIN_DATE)
```

### Dataset 3: Economic Data

**Source**:
[IMF](https://www.imf.org/external/datamapper/NGDPDPC@WEO/OEMDC/ADVEC/WEOWORLD).

**Purpose**

The International Monetary Fund (IMF) data is collected by the IMF
itself. It gathers data from its member countries and other sources to
monitor economic and financial developments globally. The data is
typically collected through various means, including direct reporting
from member countries, surveys, and other sources such as international
organizations and financial institutions.

**Format**

The format of the IMF data can vary depending on the specific dataset,
but it often includes structured data tables, time series data, and
reports. IMF includes Excel spreadsheets, CSV files, databases, and
interactive online tools, to accommodate different user needs and
preferences.

**Cleaning**

To clean and process H1B visa data, we used population data from the years 2019 to 2023.
This is reported by countries. Total population per country.

When we load the dataset, we need to extract the column headers.

This is written to `dataset/IMF_data/imf_population.csv`.

We also used GDP per capita in U.S dollars, unemployment rate per country as a percentage.

There is not much cleaning to be done since these informations can be directly extract from the datasets.

### Dependencies Used For Data.qmd

Below is a list of packages that we use for our dataset

| Library     | Description                                         |
|-------------|-----------------------------------------------------|
| dplyr       | Provides tools for data manipulation.               |
| tsibble     | Provides time series data structures and functions. |
| tidyr       | Helps in tidying data for analysis.                 |
| ggplot2     | A powerful data visualization system.               |
| forecast    | Methods and tools for forecasting time series data. |
| zoo         | Methods for irregular time series data.             |
| magick      | Image processing with ImageMagick.                  |
| viridis     | Color palettes for data visualization.              |
| cluster     | Functions for cluster analysis.                     |
| broom       | Converts analysis objects into tidy data frames.    |
| stringr     | String manipulation functions.                      |
| fastDummies | Creates dummy columns for categorical variables.    |
| prophet     | Time series forecasting using an additive model.    |
| ggfortify   | Extends ggplot2 for time series and other objects.  |
| lubridate   | Facilitates working with dates and times.           |
| corrplot    | Visualizes correlation matrices.                    |
