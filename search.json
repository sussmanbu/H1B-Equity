[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data. Here is a an example of a very simple one.\n\n# run this to load and clean the data and save it as an rds file\nsource(\n  \"scripts/load_and_clean_data.R\",\n  echo = FALSE\n)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n# run this to load the dataset from the rds\ndata &lt;- readRDS(file = \"dataset/cleaned_h1b_data.rds\")\n# your code here\ndata\n\n# A tibble: 199 × 28\n   Country Continent  FY97  FY98  FY99  FY00  FY01  FY02  FY03  FY04  FY05  FY06\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Algeria Africa       52    49    53    64    75    43    17    36    28    21\n 2 Angola  Africa        0     0     2     2     2     4     6     8     5    10\n 3 Benin   Africa       10     4     5     8    13     8     7    13     7     6\n 4 Botswa… Africa        2     5     5     3     9     8    11    21    12    16\n 5 Burkin… Africa        3     6     8     4     6    13    16    14    12    16\n 6 Burundi Africa        1     1     1     1     0     0     1     0     2     1\n 7 Cabo V… Africa       NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n 8 Camero… Africa       36    50    62    59    80    79    64    68    63    77\n 9 Centra… Africa        3     0     2     1     0     6     2     2     1     0\n10 Chad    Africa        0     4     4     1     2     1     3     4     6    10\n# ℹ 189 more rows\n# ℹ 16 more variables: FY07 &lt;dbl&gt;, FY08 &lt;dbl&gt;, FY09 &lt;dbl&gt;, FY10 &lt;dbl&gt;,\n#   FY11 &lt;dbl&gt;, FY12 &lt;dbl&gt;, FY13 &lt;dbl&gt;, FY14 &lt;dbl&gt;, FY15 &lt;dbl&gt;, FY16 &lt;dbl&gt;,\n#   FY17 &lt;dbl&gt;, FY18 &lt;dbl&gt;, FY19 &lt;dbl&gt;, FY20 &lt;dbl&gt;, FY21 &lt;dbl&gt;, FY22 &lt;dbl&gt;\n\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\nDescribe the different data files used and what each variable means.\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nResponse\nOur original data files is downloaded from the travel.state.gov website and stored in the dataset folder. The data was collected by U.S Department of State, more specifically the Bureau of Consular Affairs.\nThe Report of the Visa Office is an annual report providing statistical information on immigrant and non-immigrant visa issuance by consular offices, including information visa allocation in numerically limited categories. The main part of this dataset we want to focus on is the country of origin of visa recipients for H1B-type visas. Years covered range from Fiscal Year 1997 to F.Y. 2022 inclusively.\n\nThe original dataset comes with the following features/columns\ncountry: origin of country\nH1B: H1B approval count\nFY97 - FY22: represent year 1997 till 2022\nEssentially, this dataset contains information of H1B approval from year 1997 till 2022 and its originate country.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc.\n\n\n\nResponse\nThe original dataset is in dataset directory as .xlsx file. It contains pivot tables for each year that describe various U.S travel approvals such as H1B. To transform the dataset into the current one, we did the following operation\nread in excel files\nfilter out null values(continents, .., etc)\ngroup country\nfilter out other columns and keep H1B column\ncreate columns for each year (1977-2022)\nThe script to transform the dataset is in scripts directory load_and_clean_data.R file.\nThis is an example of how to load our cleaned dataset from dataset directory:\n\nsuppressPackageStartupMessages(library(tidyverse))\n#filename &lt;- file.choose(\"cleaned_h1b_data.rds\")\n#print(filename)\nh1b_country &lt;- \n  read_rds('dataset/cleaned_h1b_data.rds')\n\n\n#changing the year to numerical values\ncurrent_names &lt;- names(h1b_country)\nprint(current_names)\n\n [1] \"Country\"   \"Continent\" \"FY97\"      \"FY98\"      \"FY99\"      \"FY00\"     \n [7] \"FY01\"      \"FY02\"      \"FY03\"      \"FY04\"      \"FY05\"      \"FY06\"     \n[13] \"FY07\"      \"FY08\"      \"FY09\"      \"FY10\"      \"FY11\"      \"FY12\"     \n[19] \"FY13\"      \"FY14\"      \"FY15\"      \"FY16\"      \"FY17\"      \"FY18\"     \n[25] \"FY19\"      \"FY20\"      \"FY21\"      \"FY22\"     \n\n#names(df) &lt;- new_names\n\nThe code below is to rename the columns to understandable years format.\n\ncolnames(h1b_country) &lt;- c('Country','Continent','1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022') \n\nh1b_country\n\n# A tibble: 199 × 28\n   Country     Continent `1997` `1998` `1999` `2000` `2001` `2002` `2003` `2004`\n   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Algeria     Africa        52     49     53     64     75     43     17     36\n 2 Angola      Africa         0      0      2      2      2      4      6      8\n 3 Benin       Africa        10      4      5      8     13      8      7     13\n 4 Botswana    Africa         2      5      5      3      9      8     11     21\n 5 Burkina Fa… Africa         3      6      8      4      6     13     16     14\n 6 Burundi     Africa         1      1      1      1      0      0      1      0\n 7 Cabo Verde  Africa        NA     NA     NA     NA     NA     NA     NA     NA\n 8 Cameroon    Africa        36     50     62     59     80     79     64     68\n 9 Central Af… Africa         3      0      2      1      0      6      2      2\n10 Chad        Africa         0      4      4      1      2      1      3      4\n# ℹ 189 more rows\n# ℹ 18 more variables: `2005` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;,\n#   `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;,\n#   `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n\nThe following code shortens the dimension of our dataframe to 3 columns specially the years columns. Having many years column can make the dataset hard to analysis or explore.\n\nh1b_continent &lt;- h1b_country |&gt;\n  group_by(Continent)|&gt;\n  select(-Country)|&gt;\n  summarize(across(everything(), sum, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarize()`.\nℹ In argument: `across(everything(), sum, na.rm = TRUE)`.\nℹ In group 1: `Continent = \"Africa\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nlibrary(tidyr)\ndata_long &lt;- pivot_longer(h1b_continent, cols = -Continent, names_to = \"year\", values_to = \"count_h1b\")\n\ndata_long\n\n# A tibble: 182 × 3\n   Continent year  count_h1b\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 Africa    1997       2518\n 2 Africa    1998       2634\n 3 Africa    1999       2835\n 4 Africa    2000       3126\n 5 Africa    2001       3808\n 6 Africa    2002       2903\n 7 Africa    2003       2456\n 8 Africa    2004       3022\n 9 Africa    2005       2794\n10 Africa    2006       2886\n# ℹ 172 more rows\n\n\n\nThis page should be self-contained."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nblogpost5\n\n\n\n\n\nThis blog post will build on blogpost4 to merge the 3rd dataset by case numbers\n\n\n\n\n\n\nApr 10, 2024\n\n\nteam3\n\n\n\n\n\n\n  \n\n\n\n\nblog-post-4\n\n\n\n\n\nThis post details blog-post-4\n\n\n\n\n\n\nApr 7, 2024\n\n\nteam3\n\n\n\n\n\n\n  \n\n\n\n\nblog-post-3\n\n\n\n\n\nThis post details basic explore data exploration.\n\n\n\n\n\n\nApr 5, 2024\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 2\n\n\n\n\n\nThis post details specifics of our dataset and our initial processing steps.\n\n\n\n\n\n\nMar 25, 2024\n\n\nT3am-3\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 1\n\n\n\n\n\nThis post details specifics of our dataset and our initial processing steps.\n\n\n\n\n\n\nMar 4, 2024\n\n\nT3am-3\n\n\n\n\n\n\n  \n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\ntest_1\n\n\n\n\n\ntest_1 2/23/24\n\n\n\n\n\n\nFeb 23, 2024\n\n\nweiningmai\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-03-23-blogpost2/blogpost2.html",
    "href": "posts/2024-03-23-blogpost2/blogpost2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "Data background\n\nThe data we will analyze is from the U.S Department of State, more specifically the Bureau of Consular Affairs. The Report of the Visa Office is an annual report providing statistical information on immigrant and non-immigrant visa issuances by consular offices, including information visa allocation in numerically limited categories. The main part of this dataset we want to focus on is the country of origin of visa recipients for H1B-type visas. Years covered range from Fiscal Year 1997 to F.Y. 2022 inclusively.\nRegarding potential biases and issues, the data includes all types of visa applicants processed by U.S. consular offices, which could cause biases such as those related to the demographics of applicants or the economic conditions in their countries of origin. These factors might change the likelihood of type of visa for which individuals apply. The data are collected for the purpose of providing transparent information regarding the visa operations and to assist in policy making, and research within government agencies as well as for public knowledge. Since FY2019, the methodology for calculating visa data improved for greater accuracy.\n\nData Loading and Cleaning\n\nThe raw data is a 2.3MB excel file downloaded from the travel.state.gov website and stored in the dataset folder. Our load_and_clean_data.R script currently reshapes the data from a 3D country-by-visa-type-by-year format to a 2D country-by-year format where only H1B-type visas are considered. The stored data format is .rds.\nThe original data also includes continent names and totals entered as additional rows before and after every group of countries, which are arranged according to their continent. Our script allocates a continent value to each country based on row order relative to those rows, represented as its own Continent column. The rows containing continent names and totals are then removed. It is also noteworthy that an Unknown continent value exists for the two rows No Nationality and United Nations Laissez-Passer, which denote the two special cases for which US visas may be given without being linked to any specific country’s passport.\n\nData Equity\n\nAcquisition - the dataset comes from the travel state government. It reports statistical information on immigrants and non immigrants visas. Although officials did not explicitly mention the purpose of collecting these visa data annually, it is appropriate to infer that the data serves security and transparency purposes since the travel state government is responsible to provide visa services to foreign nationals who wish to stay in the U.S.\nConception - Because our dataset contains H1B approval from origin countries, it shows immigrant trends and labor market dynamics. For example, governments may analyze H1B approval data to evaluate the effectiveness of immigrant policies attracting talents."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-03-04-blog-post-1/blog-post-1.html",
    "href": "posts/2024-03-04-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "The first dataset we looked at is from the US Citizenship and Immigration Services (USCIS) website: https://www.uscis.gov/tools/reports-and-studies/h-1b-employer-data-hub/h-1b-employer-data-hub-files\n\nIt contains 15 csv files ranging from years 2009 to 2023 with about 30,000 rows per csv file and 10 columns including the fiscal year. Looking at the columns of this dataset, it seems it was collected to see how many H1B goes through each year with number of denials, tax id, and the city applicants in. Because this is a tabular dataset, it is possible to load in R easily and start exploration data analysis. One challenge of working with dataset is the fact that it might not help us answer deep questions. For example, if we want to answer questions such as what companies, salary incomes(plotting salary distributions versus other features), ethnicity groups, and gender.\nNote: This same dataset can be found from U.S citizenship immigration services, but someone uploaded to Kaggle.\n\nThe second dataset[s] we will analyze is from travel.state.gov and can be found at: travel.state.gov\n\nThe data includes rows of countries of origin, separated by continent. The columns include Visa type and total visas issued. There is data that includes the month of the issuance as well.\nThis data was published by the Visa Office of the US on the travel.state.gov website. The main question this dataset addresses is the country of origin of visa recipients.\nWe will load the data by using optical character recognition and combine the datasets from each year to form one large dataset. This larger dataset will have multiple rows for each country, one for each year from 2000-2023(maybe month as well) and can be used to evaluate changes in issuances by country over time.\n\nIt may be challenging to load the data because it could be tedious. Also, this data will be difficult to join with other datasets because the most ‘relevant’ joining column will be month/year of issuance, which is not a very specific identifier for this data.\n\nThe first dataset we looked at is from the US Citizenship and Immigration Services (USCIS) website: https://www.uscis.gov/tools/reports-and-studies/h-1b-employer-data-hub/h-1b-employer-data-hub-files\n\nIt contains 15 csv files ranging from years 2009 to 2023 with about 30,000 rows per csv file and 10 columns including the fiscal year. Looking at the columns of this dataset, it seems it was collected to see how many H1B goes through each year with number of denials, tax id, and the city applicants in. Because this is a tabular dataset, it is possible to load in R easily and start exploration data analysis. One challenge of working with dataset is the fact that it might not help us answer deep questions. For example, if we want to answer questions such as what companies, salary incomes(plotting salary distributions versus other features), ethnicity groups, and gender.\nNote: This same dataset can be found from U.S citizenship immigration services, but someone uploaded to Kaggle.\n\nThe second dataset[s] we will analyze is from travel.state.gov and can be found at: travel.state.gov\n\nThe data includes rows of countries of origin, separated by continent. The columns include Visa type and total visas issued. There is data that includes the month of the issuance as well.\nThis data was published by the Visa Office of the US on the travel.state.gov website. The main question this dataset addresses is the country of origin of visa recipients.\nWe will load the data by using optical character recognition and combine the datasets from each year to form one large dataset. This larger dataset will have multiple rows for each country, one for each year from 2000-2023(maybe month as well) and can be used to evaluate changes in issuances by country over time.\nIt may be challenging to load the data because it could be tedious. Also, this data will be difficult to join with other datasets because the most ‘relevant’ joining column will be month/year of issuance, which is not a very specific identifier for this data.\n\nThe last dataset we plan to use is from the US Department of Labor: https://www.dol.gov/agencies/eta/foreign-labor/performance\n\nIt contains detailed performance data on H-1B applications from 2008 to 2023, with approximately 600,000 to 630,000 rows per CSV file and 26 columns. Each row represents one H-1B applicant with their case number. The 26 columns include information such as applicants’ job titles, application dates, wage levels, postal codes, etc. This data was published by the US Department of Labor. We will load the data in R, as all the data are saved in CSV files and organized by year. This dataset contains very precise information, and we want to focus on the applicants’ job titles, wage levels, and their postal codes to find out the economic impact of H-1B visas. One possible challenge I notice is that the dataset contains a huge amount of data; it’s important for us to filter out irrelevant information.\n\n\n\n\n\n\n\n8d31b065024cb4260dfb626578ba4a385d579013"
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html",
    "title": "blog-post-4",
    "section": "",
    "text": "readxl"
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html#packages-used",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html#packages-used",
    "title": "blog-post-4",
    "section": "",
    "text": "readxl"
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html#continuation-work",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html#continuation-work",
    "title": "blog-post-4",
    "section": "Continuation work",
    "text": "Continuation work\nIn blog posts 2 and 3, we uncovered a noticeable trend in our second dataset. We analyzed the H1B visa approval rates across different countries by extracting data from an Excel file. One key finding was that Asian countries have the highest H1B approval counts compared to other regions. This discovery led us to delve deeper into understanding the reasons behind this trend and to explore the interconnections with other variables.\nHowever, we faced a challenge with the second dataset as it limited our ability to gain deeper insights or construct statistical models. To overcome this and enrich our analysis, we decided to integrate additional datasets from blog post 1. This integration aims to provide a more comprehensive view and enable a better exploration of the underlying factors influencing the H1B approval trends."
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html#dataset-1",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html#dataset-1",
    "title": "blog-post-4",
    "section": "Dataset 1",
    "text": "Dataset 1\nFor our first dataset, we have the following columns\nFiscal Year: The fiscal year of the H-1B application.\nEmployer: The name of the employer applying for the H-1B visa.\nInitial Approval: The number of initial H-1B applications approved.\nInitial Denial: The number of initial H-1B applications denied.\nContinuing Approval: The number of continuing H-1B applications approved.\nContinuing Denial: The number of continuing H-1B applications denied.\nNAICS: The North American Industry Classification System code.\nTax ID: The Tax Identification Number of the employer.\nState: The state where the employer is located.\nCity: The city where the employer is located.\nZIP: The ZIP code where the employer is located\nWith this dataset, it seems like we can calculate approval and denial rates by employer. We also wonder of the possibility to predict approval/denial with logistic regression. However, we think that the features/columns in this dataset do not represent importances to help classify approval or denial."
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html#dataset-3",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html#dataset-3",
    "title": "blog-post-4",
    "section": "Dataset 3",
    "text": "Dataset 3\nWith this dataset, it seems like we have a lot of features to explore. However, we can filter columns that contribute to the predicator variable we care about. Specifically, we will explore the most recent year dataset if we want to build statistical model on it. It does not make sense to build a statistical model on year 2010 and use it to explain current year since data distribution is not the same. For this blog post, we decide to start on the year 2024 first to do our exploration and initial modeling. Note that modeling is an iterative process.\n\nlibrary(readxl)\nlibrary(readr)\n\n\nInitial thought\nLooking at this dataset, we want to filter out columns and explore relationships in dataset. Because the original dataset exceeds the limit for Github store without Git LFS, we will saved the cleaned and fewer columns dataset.\nColumns to consider\ncase_number\ncase_status\nreceived_date/decision_date\nVisa-class: Focus only H1B\nJob_title\nfulltime_position\nbegin_date/end_date\nnew_employment\ncontinued_employment\nemployer name: company name\nemployer state\nAgent_representing_employer\njoin with additional dataset with more columns:\nprevailing wage and name of study/colleges\nWe decide to look into these columns since we think that these are high feature importances in terms of making predictive model. Also, we have a hypothesis that a good agent(lawyer) can also increase acceptance of H1B.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndf &lt;- readRDS(\"dataset/cleaned_disclosure.rds\")\nprint(df)\n\n# A tibble: 101,179 × 12\n   CASE_NUMBER    CASE_STATUS RECEIVED_DATE       DECISION_DATE       VISA_CLASS\n   &lt;chr&gt;          &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;     \n 1 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 2 I-203-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 E-3 Austr…\n 3 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 4 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 5 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 6 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 7 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 8 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 9 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n10 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n# ℹ 101,169 more rows\n# ℹ 7 more variables: JOB_TITLE &lt;chr&gt;, FULL_TIME_POSITION &lt;chr&gt;,\n#   BEGIN_DATE &lt;dttm&gt;, END_DATE &lt;dttm&gt;, EMPLOYER_NAME &lt;chr&gt;,\n#   EMPLOYER_STATE &lt;chr&gt;, AGENT_REPRESENTING_EMPLOYER &lt;chr&gt;\n\n\n\nnew_df &lt;- df |&gt;\n  filter(\n    VISA_CLASS %in% c(\n      \"H-1B\"\n    )\n  )\n\n\nnew_df$FULL_TIME_POSITION &lt;- factor(new_df$FULL_TIME_POSITION)\n\nnew_df$AGENT_REPRESENTING_EMPLOYER &lt;- factor(new_df$AGENT_REPRESENTING_EMPLOYER)\nmylogit &lt;- glm(FULL_TIME_POSITION ~ AGENT_REPRESENTING_EMPLOYER, data = new_df, family = \"binomial\")\n\n\nsummary(mylogit)\n\n\nCall:\nglm(formula = FULL_TIME_POSITION ~ AGENT_REPRESENTING_EMPLOYER, \n    family = \"binomial\", data = new_df)\n\nCoefficients:\n                               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     4.27569    0.05054  84.606  &lt; 2e-16 ***\nAGENT_REPRESENTING_EMPLOYERYes -0.44945    0.05702  -7.883  3.2e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 18444  on 97692  degrees of freedom\nResidual deviance: 18377  on 97691  degrees of freedom\nAIC: 18381\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n# model 2\nnew_df$FULL_TIME_POSITION &lt;- factor(new_df$FULL_TIME_POSITION)\n\nnew_df$AGENT_REPRESENTING_EMPLOYER &lt;- factor(new_df$AGENT_REPRESENTING_EMPLOYER)\nnew_df$EMPLOYER_STATE &lt;- factor(new_df$EMPLOYER_STATE)\nmylogit &lt;- glm(FULL_TIME_POSITION ~ AGENT_REPRESENTING_EMPLOYER+EMPLOYER_STATE, data = new_df, family = \"binomial\")\n\n\nsummary(mylogit)\n\n\nCall:\nglm(formula = FULL_TIME_POSITION ~ AGENT_REPRESENTING_EMPLOYER + \n    EMPLOYER_STATE, family = \"binomial\", data = new_df)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       4.14039    0.71619   5.781 7.42e-09 ***\nAGENT_REPRESENTING_EMPLOYERYes   -0.19105    0.06156  -3.103  0.00191 ** \nEMPLOYER_STATEAL                 -1.85110    0.74385  -2.489  0.01283 *  \nEMPLOYER_STATEAR                  0.53670    0.79745   0.673  0.50094    \nEMPLOYER_STATEAZ                  0.13182    0.76300   0.173  0.86284    \nEMPLOYER_STATECA                  0.02224    0.71596   0.031  0.97522    \nEMPLOYER_STATECO                 -0.20357    0.76701  -0.265  0.79069    \nEMPLOYER_STATECT                 -0.28111    0.75499  -0.372  0.70965    \nEMPLOYER_STATEDC                  0.13100    0.84432   0.155  0.87670    \nEMPLOYER_STATEDE                 -0.30400    0.77640  -0.392  0.69539    \nEMPLOYER_STATEFL                 -0.57434    0.72425  -0.793  0.42777    \nEMPLOYER_STATEFM                 11.42568 1029.12171   0.011  0.99114    \nEMPLOYER_STATEGA                 -0.66283    0.72117  -0.919  0.35805    \nEMPLOYER_STATEGU                 11.53518  121.21811   0.095  0.92419    \nEMPLOYER_STATEHI                 -0.57196    1.24162  -0.461  0.64505    \nEMPLOYER_STATEIA                 -0.78786    0.76794  -1.026  0.30492    \nEMPLOYER_STATEID                 -0.52818    0.92393  -0.572  0.56755    \nEMPLOYER_STATEIL                  0.24894    0.72454   0.344  0.73116    \nEMPLOYER_STATEIN                 -0.09901    0.77619  -0.128  0.89850    \nEMPLOYER_STATEKS                 -0.95367    0.77264  -1.234  0.21709    \nEMPLOYER_STATEKY                  0.38863    0.92048   0.422  0.67288    \nEMPLOYER_STATELA                 -0.83444    0.79088  -1.055  0.29139    \nEMPLOYER_STATEMA                 -0.78057    0.71859  -1.086  0.27737    \nEMPLOYER_STATEMD                  0.62599    0.74275   0.843  0.39934    \nEMPLOYER_STATEME                 -0.81796    0.87756  -0.932  0.35129    \nEMPLOYER_STATEMI                 -0.33243    0.72313  -0.460  0.64572    \nEMPLOYER_STATEMN                 -0.30756    0.73904  -0.416  0.67729    \nEMPLOYER_STATEMO                  0.53091    0.77557   0.685  0.49363    \nEMPLOYER_STATEMP                 -1.12560    1.25132  -0.900  0.36837    \nEMPLOYER_STATEMS                 -0.83378    0.92580  -0.901  0.36780    \nEMPLOYER_STATEMT                 11.60222  239.22940   0.048  0.96132    \nEMPLOYER_STATENC                  1.22779    0.76610   1.603  0.10901    \nEMPLOYER_STATEND                  0.37102    1.23386   0.301  0.76364    \nEMPLOYER_STATENE                  2.78154    1.00639   2.764  0.00571 ** \nEMPLOYER_STATENH                 -0.61786    0.87649  -0.705  0.48086    \nEMPLOYER_STATENJ                  0.54768    0.72064   0.760  0.44726    \nEMPLOYER_STATENM                  0.94555    1.23160   0.768  0.44264    \nEMPLOYER_STATENV                 -0.22182    0.87489  -0.254  0.79985    \nEMPLOYER_STATENY                 -0.81520    0.71667  -1.137  0.25534    \nEMPLOYER_STATEOH                 -0.37167    0.73214  -0.508  0.61170    \nEMPLOYER_STATEOK                  0.37214    0.79875   0.466  0.64128    \nEMPLOYER_STATEOR                 -0.72523    0.78334  -0.926  0.35454    \nEMPLOYER_STATEPA                  0.19855    0.72852   0.273  0.78520    \nEMPLOYER_STATEPR                 11.57417  352.84919   0.033  0.97383    \nEMPLOYER_STATERI                  0.22732    0.84415   0.269  0.78771    \nEMPLOYER_STATESC                 -0.53251    0.79013  -0.674  0.50034    \nEMPLOYER_STATESD                 -2.01863    0.79765  -2.531  0.01138 *  \nEMPLOYER_STATETN                 -0.08546    0.74711  -0.114  0.90893    \nEMPLOYER_STATETX                  0.21922    0.71860   0.305  0.76032    \nEMPLOYER_STATEUT                  0.12953    0.84435   0.153  0.87807    \nEMPLOYER_STATEVA                 -0.24928    0.72214  -0.345  0.72994    \nEMPLOYER_STATEVI                 11.52463  727.28733   0.016  0.98736    \nEMPLOYER_STATEVT                  0.61491    0.87447   0.703  0.48194    \nEMPLOYER_STATEWA                  1.17962    0.73693   1.601  0.10944    \nEMPLOYER_STATEWI                  0.10976    0.77589   0.141  0.88751    \nEMPLOYER_STATEWV                 -2.07776    0.89213  -2.329  0.01986 *  \nEMPLOYER_STATEWY                 -1.83357    0.83402  -2.198  0.02791 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 18443  on 97679  degrees of freedom\nResidual deviance: 17775  on 97623  degrees of freedom\n  (13 observations deleted due to missingness)\nAIC: 17889\n\nNumber of Fisher Scoring iterations: 14\n\n\nFor AGENT_REPRESENTING_EMPLOYERYes, the coefficient is -0.19105, indicating that when the agent is representing the employer, the log-odds of having a full-time position (“Y”) decrease compared to when the agent is not representing (the baseline). This contradicts to our assumption that having a good lawyer increases the chance of H1B approval. Perhaps, it is worth investigate further.\nFor state, MA(Massachusetts) and NY(New York) have a similiar coefficient, that is, negative effect on the likelihood of getting full-time position.\nOverall, we can see that there exist features in dataset that contribute to whether likely to being offer fulltime position or not. If possible, we can join the remaining two datasets with case number which gives us more features to work on."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-02-23-test1/test1.html",
    "href": "posts/2024-02-23-test1/test1.html",
    "title": "test_1",
    "section": "",
    "text": "testing"
  },
  {
    "objectID": "posts/2024-04-10-blogpost5/blogpost5.html",
    "href": "posts/2024-04-10-blogpost5/blogpost5.html",
    "title": "blogpost5",
    "section": "",
    "text": "library(readxl)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\ndf &lt;- readRDS(\"dataset/cleaned_disclosure.rds\")\nprint(df)\n\n# A tibble: 101,179 × 12\n   CASE_NUMBER    CASE_STATUS RECEIVED_DATE       DECISION_DATE       VISA_CLASS\n   &lt;chr&gt;          &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;     \n 1 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 2 I-203-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 E-3 Austr…\n 3 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 4 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 5 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 6 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 7 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 8 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 9 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n10 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n# ℹ 101,169 more rows\n# ℹ 7 more variables: JOB_TITLE &lt;chr&gt;, FULL_TIME_POSITION &lt;chr&gt;,\n#   BEGIN_DATE &lt;dttm&gt;, END_DATE &lt;dttm&gt;, EMPLOYER_NAME &lt;chr&gt;,\n#   EMPLOYER_STATE &lt;chr&gt;, AGENT_REPRESENTING_EMPLOYER &lt;chr&gt;\n\n\n\n# rds for the other two projects\n# df2 &lt;- read_excel(\"dataset/worksite.xlsx\")\n# saveRDS(df2, \"dataset/LCA_worksite.rds\")\n\n# df3 &lt;- read_excel(\"dataset/appendix.xlsx\")\n# saveRDS(df3, \"dataset/LCA_appendix.rds\")\n\n\ndf2 &lt;- readRDS(\"dataset/LCA_worksite.rds\")\n\nprint(df2)\n\n# A tibble: 149,649 × 22\n   CASE_NUMBER        WORKSITE_WORKERS SECONDARY_ENTITY SECONDARY_ENTITY_BUSIN…¹\n   &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;                   \n 1 I-200-19297-106681                1 N                &lt;NA&gt;                    \n 2 I-200-19319-148860                1 N                &lt;NA&gt;                    \n 3 I-200-19319-148860                1 N                &lt;NA&gt;                    \n 4 I-200-19319-148860                1 Y                Acuity Eyecare Group    \n 5 I-200-19319-149308                1 N                &lt;NA&gt;                    \n 6 I-200-19323-154281                1 N                &lt;NA&gt;                    \n 7 I-200-19345-192268                1 N                &lt;NA&gt;                    \n 8 I-200-20007-237296                1 N                &lt;NA&gt;                    \n 9 I-200-20007-237296                1 N                &lt;NA&gt;                    \n10 I-200-20009-243900                1 Y                Florida Department of C…\n# ℹ 149,639 more rows\n# ℹ abbreviated name: ¹​SECONDARY_ENTITY_BUSINESS_NAME\n# ℹ 18 more variables: WORKSITE_ADDRESS1 &lt;chr&gt;, WORKSITE_ADDRESS2 &lt;chr&gt;,\n#   WORKSITE_CITY &lt;chr&gt;, WORKSITE_COUNTY &lt;chr&gt;, WORKSITE_STATE &lt;chr&gt;,\n#   WORKSITE_POSTAL_CODE &lt;dbl&gt;, WAGE_RATE_OF_PAY_FROM &lt;dbl&gt;,\n#   WAGE_RATE_OF_PAY_TO &lt;dbl&gt;, WAGE_UNIT_OF_PAY &lt;chr&gt;, PREVAILING_WAGE &lt;dbl&gt;,\n#   PW_UNIT_OF_PAY &lt;chr&gt;, PW_TRACKING_NUMBER &lt;chr&gt;, PW_WAGE_LEVEL &lt;chr&gt;, …\n\n\n\ndf3 &lt;- readRDS(\"dataset/appendix.rds\")\n\nprint(df3)\n\n# A tibble: 92 × 5\n   CASE_NUMBER        APPX_A_NO_OF_EXEMPT_WORKERS APPX_A_NAME_OF_INSTITUTION    \n   &lt;chr&gt;                                    &lt;dbl&gt; &lt;chr&gt;                         \n 1 I-200-20353-968632                           1 The William Paterson Universi…\n 2 I-200-21116-258512                           1 GOVERNORS STATE UNIVERISTY    \n 3 I-200-22115-102220                           1 Lamar University              \n 4 I-200-22126-145447                           1 California State University, …\n 5 I-200-22164-272276                           1 Illinois Institute of Technol…\n 6 I-200-22180-320955                           1 University of Missouri        \n 7 I-200-22180-320965                           1 Columbia University           \n 8 I-200-22180-320973                           1 Illinois Institute of Technol…\n 9 I-200-22194-349007                           1 University of Missouri        \n10 I-200-22214-390922                           1 Northeastern University       \n# ℹ 82 more rows\n# ℹ 2 more variables: APPX_A_FIELD_OF_STUDY &lt;chr&gt;, APPX_A_DATE_OF_DEGREE &lt;dttm&gt;"
  },
  {
    "objectID": "posts/2024-03-23-blogpost3/blogpost3.html",
    "href": "posts/2024-03-23-blogpost3/blogpost3.html",
    "title": "blog-post-3",
    "section": "",
    "text": "Further cleaning: we discovered that one of the rows of the Unknown continent, United Nations Laissez-Passer, contained only empty rows. This suggests that this Country value might have had some nonzero values for some non-H1B visa types in the original dataset, but since it has zero H1B visas given to it over all the years, we deleted this row.\nData exploration: [comment on data exploration here, explaining the following 3 graphs:]\n\n\n# run this if you don't have the magick package installed\n# install.packages(\"magick\")\n# make sure to comment install.package out before creating gh-pages.\n\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\nimage1 &lt;- image_read('images/total_bar_stacked.png')\nplot(image1)\n\n\n\n\nObservation: we see a positive trend on the number of H1B approvals. As the year progresses, the number of H1B approvals continue to rise. However, we notice that there exist a sharp drop in year 2020 and 2021 particularly due to covid. Next, it spikes back up in year 2022. From this plot, we also see that Asia has been the consistent country where H1B approvals happen the most.\n\nimage2 &lt;- image_read('images/total_bar.png')\nplot(image2)\n\n\n\n\nFor this side by side comparison of H1B approvals and other countries, we get to observe the trend of each countries. It seems like Asia has been getting positive trends since year 1997 till 2022 except 2020 and 2021 due to covid. In contrast, other countries like Europe, South America, Oceania and Africa have all been trending downward for H1B approval. It can be worth to understand what factors contribute to the trends.\n\nimage3 &lt;- image_read('images/percent_bar_faceted.png')\nplot(image3)\n\n\n\n\nThe last plot shows similar content to plot 2. However, with percentage being display, we see that Asia out numbered all other countries combined in terms of H1B approval and Europe comes in second place."
  },
  {
    "objectID": "DataExp.html",
    "href": "DataExp.html",
    "title": "MA [46]15 Final Project T3am-3",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\nsuppressPackageStartupMessages(library(tidyverse))\n#filename &lt;- file.choose(\"cleaned_h1b_data.rds\")\n#print(filename)\nh1b_country &lt;- \n  read_rds('dataset/cleaned_h1b_data.rds')\n\n\n#changing the year to numerical values\ncurrent_names &lt;- names(h1b_country)\nprint(current_names)\n\n [1] \"Country\"   \"Continent\" \"FY97\"      \"FY98\"      \"FY99\"      \"FY00\"     \n [7] \"FY01\"      \"FY02\"      \"FY03\"      \"FY04\"      \"FY05\"      \"FY06\"     \n[13] \"FY07\"      \"FY08\"      \"FY09\"      \"FY10\"      \"FY11\"      \"FY12\"     \n[19] \"FY13\"      \"FY14\"      \"FY15\"      \"FY16\"      \"FY17\"      \"FY18\"     \n[25] \"FY19\"      \"FY20\"      \"FY21\"      \"FY22\"     \n\n#names(df) &lt;- new_names\n\n\ncolnames(h1b_country) &lt;- c('Country','Continent','1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022') \n\nh1b_country\n\n# A tibble: 199 × 28\n   Country     Continent `1997` `1998` `1999` `2000` `2001` `2002` `2003` `2004`\n   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Algeria     Africa        52     49     53     64     75     43     17     36\n 2 Angola      Africa         0      0      2      2      2      4      6      8\n 3 Benin       Africa        10      4      5      8     13      8      7     13\n 4 Botswana    Africa         2      5      5      3      9      8     11     21\n 5 Burkina Fa… Africa         3      6      8      4      6     13     16     14\n 6 Burundi     Africa         1      1      1      1      0      0      1      0\n 7 Cabo Verde  Africa        NA     NA     NA     NA     NA     NA     NA     NA\n 8 Cameroon    Africa        36     50     62     59     80     79     64     68\n 9 Central Af… Africa         3      0      2      1      0      6      2      2\n10 Chad        Africa         0      4      4      1      2      1      3      4\n# ℹ 189 more rows\n# ℹ 18 more variables: `2005` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;,\n#   `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;,\n#   `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n\n\nh1b_continent &lt;- h1b_country |&gt;\n  group_by(Continent)|&gt;\n  select(-Country)|&gt;\n  summarize(across(everything(), sum, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarize()`.\nℹ In argument: `across(everything(), sum, na.rm = TRUE)`.\nℹ In group 1: `Continent = \"Africa\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nlibrary(tidyr)\ndata_long &lt;- pivot_longer(h1b_continent, cols = -Continent, names_to = \"year\", values_to = \"count_h1b\")\n\ndata_long\n\n# A tibble: 182 × 3\n   Continent year  count_h1b\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 Africa    1997       2518\n 2 Africa    1998       2634\n 3 Africa    1999       2835\n 4 Africa    2000       3126\n 5 Africa    2001       3808\n 6 Africa    2002       2903\n 7 Africa    2003       2456\n 8 Africa    2004       3022\n 9 Africa    2005       2794\n10 Africa    2006       2886\n# ℹ 172 more rows\n\n\n\nggplot(data_long, aes(x = year, y = count_h1b, fill = Continent)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  labs(title = \"Total Values by Continent Over the Years\",\n       x = \"Year\", y = \"Total Value\") + theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nggplot(data_long, aes(x = year, y = count_h1b, fill = Continent)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  facet_wrap(~Continent, scales='free_y') + \n  labs(title = \"Total Values by Continent Over the Years\",\n       x = \"Year\", y = \"Total Value\") + theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_x_discrete(guide=guide_axis(check.overlap=TRUE))\n\n\n\n\n\nh1b_continent &lt;- h1b_country |&gt;\n  filter(Continent == 'Unknown') |&gt;\n  select(-Continent)\n\n# library(tidyr)\nunknown_longer &lt;- pivot_longer(h1b_continent, cols = -Country, names_to = \"year\", values_to = \"count_h1b\")\n# \nunknown_longer\n\n# A tibble: 26 × 3\n   Country        year  count_h1b\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;\n 1 No Nationality 1997        209\n 2 No Nationality 1998         97\n 3 No Nationality 1999         67\n 4 No Nationality 2000         56\n 5 No Nationality 2001         52\n 6 No Nationality 2002         15\n 7 No Nationality 2003          7\n 8 No Nationality 2004         15\n 9 No Nationality 2005         13\n10 No Nationality 2006         14\n# ℹ 16 more rows\n\n\n\nnames(unknown_longer)\n\n[1] \"Country\"   \"year\"      \"count_h1b\"\n\n\n\nclean_un &lt;- unknown_longer |&gt;\n  filter(Country == 'United Nations Laissez-Passer')|&gt;\n  filter(count_h1b != 0)\n\nclean_un\n\n# A tibble: 0 × 3\n# ℹ 3 variables: Country &lt;chr&gt;, year &lt;chr&gt;, count_h1b &lt;dbl&gt;\n\n\nThis shows that no one with a nationality designated as ‘United Nations Laissez-Passer’ has received an h1b visa from the years 1997-2022, so we can clear that.\n\nggplot(unknown_longer, aes(x = year, y = count_h1b, fill = Country)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  #facet_wrap(~Continent, scales='free_y')+\n  labs(title = \"Total Values by Continent Over the Years\",\n       x = \"Year\", y = \"Total Value\") + theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nh1b_country_temp &lt;- h1b_country |&gt;\n  select(-Continent)\n\ndata_long_all &lt;- pivot_longer(h1b_country_temp, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_all &lt;- list()\n\nfor(year_iter in unique(data_long_all$year)){\n  print(year_iter)\n  all_temp &lt;- data_long_all |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- all_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- all_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_all[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_all)\n\nlist()\n\n\n\nh1b_africa &lt;- h1b_country |&gt;\n  filter(Continent == 'Africa') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_africa &lt;- pivot_longer(h1b_africa, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\n\n  africa_temp &lt;- data_long_africa |&gt; filter(year == 1997)\n  \n  #choose top 5\n  top_countries &lt;- africa_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n\nSelecting by total_count\n\n  #summarise the rest\n  other_countries &lt;- africa_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n  filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",total_count = sum(count_h1b))\n  \n  other_countries\n\n# A tibble: 1 × 2\n  Country total_count\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Other           513\n\n\n\npiecharts &lt;- list()\nfor(year_iter in unique(data_long_africa$year)){\n  africa_temp &lt;- data_long_africa |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- africa_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- africa_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 African Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts[[year_iter]] &lt;- pie_chart\n  \n}\n\nSelecting by total_count\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts)\n\nlist()\n\n\n\n#do the same for other continents\n\nh1b_asia &lt;- h1b_country |&gt;\n  filter(Continent == 'Asia') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_asia &lt;- pivot_longer(h1b_asia, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_asia &lt;- list()\n\nfor(year_iter in unique(data_long_asia$year)){\n  print(year_iter)\n  asia_temp &lt;- data_long_asia |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- asia_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- asia_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 Asian Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_asia[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_asia)\n\nlist()\n\n\n\n#do the same for other continents\nh1b_europe &lt;- h1b_country |&gt;\n  filter(Continent == 'Europe') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_europe &lt;- pivot_longer(h1b_europe, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_europe &lt;- list()\n\nfor(year_iter in unique(data_long_europe$year)){\n  print(year_iter)\n  europe_temp &lt;- data_long_europe |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- europe_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- europe_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 European Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_europe[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_europe)\n\nlist()\n\n\n\nh1b_namerica &lt;- h1b_country |&gt;\n  filter(Continent == 'North America') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_namerica &lt;- pivot_longer(h1b_namerica, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_namerica &lt;- list()\n\nfor(year_iter in unique(data_long_namerica$year)){\n  print(year_iter)\n  namerica_temp &lt;- data_long_namerica |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- namerica_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- namerica_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 North American Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_namerica[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_namerica)\n\nlist()\n\n\n\nh1b_samerica &lt;- h1b_country |&gt;\n  filter(Continent == 'South America') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_samerica &lt;- pivot_longer(h1b_samerica, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_samerica &lt;- list()\n\nfor(year_iter in unique(data_long_samerica$year)){\n  print(year_iter)\n  samerica_temp &lt;- data_long_samerica |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- samerica_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- samerica_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 South American Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_samerica[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_samerica)\n\nlist()\n\n\n\nh1b_oceania &lt;- h1b_country |&gt;\n  filter(Continent == 'Oceania') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_oceania &lt;- pivot_longer(h1b_oceania, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_oceania &lt;- list()\n\nfor(year_iter in unique(data_long_oceania$year)){\n  print(year_iter)\n  oceania_temp &lt;- data_long_oceania |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- oceania_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(3)\n  \n  #summarise the rest\n  other_countries &lt;- oceania_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 Oceanian Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_oceania[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_oceania)\n\nlist()\n\n\n\nh1b_other &lt;- h1b_country |&gt;\n  filter(Continent == 'Unknown') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_other &lt;- pivot_longer(h1b_other, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\ndata_long_other\n\n# A tibble: 26 × 3\n   Country        year  count_h1b\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;\n 1 No Nationality 1997        209\n 2 No Nationality 1998         97\n 3 No Nationality 1999         67\n 4 No Nationality 2000         56\n 5 No Nationality 2001         52\n 6 No Nationality 2002         15\n 7 No Nationality 2003          7\n 8 No Nationality 2004         15\n 9 No Nationality 2005         13\n10 No Nationality 2006         14\n# ℹ 16 more rows\n\n\n=======\n\n# Load libraries and dataset\nsuppressPackageStartupMessages(library(tidyverse))\nh1b_country &lt;- read_rds('dataset/cleaned_h1b_data.rds')\n\n\n# Rename year columns\nyears &lt;- paste0(1997:2022)\ncolnames(h1b_country) &lt;- c('Country', 'Continent', years)\n\n\n# Summarize total H1B visas per year and transform into long format for plotting\ntotal_h1b_by_year_long &lt;- h1b_country %&gt;%\n  select(all_of(years)) %&gt;%\n  summarise(across(everything(), sum, na.rm = TRUE)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"year\", values_to = \"Total_H1B_Visas\")\n\n# Plot total H1B visas issued worldwide over the years\nggplot(total_h1b_by_year_long, aes(x = year, y = Total_H1B_Visas, fill = year)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Total H1B Visas Issued Worldwide Over the Years\",\n       x = \"Year\", y = \"Total Number of H1B Visas\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1), legend.position = \"none\")\n\n\n\n\n\n# Summarize data by continent and transform into long format\ndata_long &lt;- h1b_country %&gt;%\n  group_by(Continent) %&gt;%\n  summarise(across(all_of(years), sum, na.rm = TRUE)) %&gt;%\n  pivot_longer(cols = all_of(years), names_to = \"year\", values_to = \"count_h1b\")\n\n# Plot total values by continent over the years\nggplot(data_long, aes(x = year, y = count_h1b, fill = Continent)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  facet_wrap(~Continent, scales=\"free_y\") +\n  labs(title = \"Total H1B Visas by Continent Over the Years\",\n       x = \"Year\", y = \"Total Visas\") +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n# Calculate percentages for each continent per year\ndata_long &lt;- data_long %&gt;%\n  left_join(total_h1b_by_year_long, by = \"year\") %&gt;%\n  mutate(percentage_of_total = count_h1b / Total_H1B_Visas)\n\n# Plot percentage of total values by continent over the years\nggplot(data_long, aes(x = year, y = percentage_of_total, fill = Continent)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  facet_wrap(~Continent, scales=\"free_y\") +\n  scale_y_continuous(labels = scales::percent_format(accuracy = 0.1)) +\n  labs(title = \"Percentage of Total H1B Visas by Continent Over the Years\",\n       x = \"Year\", y = \"Percentage of Total Visas\") +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n5dcbcb76829b43c5323c8d54ece3b8995ca08211"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#wei",
    "href": "about.html#wei",
    "title": "About",
    "section": "Wei",
    "text": "Wei\nSenior studying Data Science"
  },
  {
    "objectID": "about.html#xinran-sun",
    "href": "about.html#xinran-sun",
    "title": "About",
    "section": "Xinran Sun",
    "text": "Xinran Sun\nXinran is a junior majoring in Economics and Mathematics at Boston University."
  },
  {
    "objectID": "about.html#ellis-coldren",
    "href": "about.html#ellis-coldren",
    "title": "About",
    "section": "Ellis Coldren",
    "text": "Ellis Coldren\nSenior studying math and computer science in the BA/MA program."
  },
  {
    "objectID": "about.html#phil-ledoit",
    "href": "about.html#phil-ledoit",
    "title": "About",
    "section": "Phil Ledoit",
    "text": "Phil Ledoit\nPhil is a senior studying Statistics and Computer Science. His interests include data exploration, cleaning, and processing."
  },
  {
    "objectID": "about.html#ken-yusuf",
    "href": "about.html#ken-yusuf",
    "title": "About",
    "section": "Ken Yusuf",
    "text": "Ken Yusuf\nSophomore studying economics and math.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this pge should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should include a link to an interactive dashboard. The dashboard should be created either using Shiny or FlexDashboard (or another tool with professor’s approval). This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc.\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions from the Big Picture? Plotly with default hover text will get no credit. Be creative!\n\n\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]