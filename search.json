[
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "Data",
    "section": "",
    "text": "This comes from the file data.qmd.\nYour first steps in this project will be to find data to work on.\nI recommend trying to find data that interests you and that you are knowledgeable about. A bad example would be if you have no interest in video games but your data set is about video games. I also recommend finding data that is related to current events, social justice, and other areas that have an impact.\nInitially, you will study one dataset but later you will need to combine that data with another dataset. For this reason, I recommend finding data that has some date and/or location components. These types of data are conducive to interesting visualizations and analysis and you can also combine this data with other data that also has a date or location variable. Data from the census, weather data, economic data, are all relatively easy to combine with other data with time/location components."
  },
  {
    "objectID": "data.html#what-makes-a-good-data-set",
    "href": "data.html#what-makes-a-good-data-set",
    "title": "Data",
    "section": "What makes a good data set?",
    "text": "What makes a good data set?\n\nData you are interested in and care about.\nData where there are a lot of potential questions that you can explore.\nA data set that isn’t completely cleaned already.\nMultiple sources for data that you can combine.\nSome type of time and/or location component."
  },
  {
    "objectID": "data.html#where-to-keep-data",
    "href": "data.html#where-to-keep-data",
    "title": "Data",
    "section": "Where to keep data?",
    "text": "Where to keep data?\nBelow 50mb: In dataset folder\nAbove 50mb: In dataset_ignore folder. This folder will be ignored by git so you’ll have to manually sync these files across your team.\n\nSharing your data\nFor small datasets (&lt;50mb), you can use the dataset folder that is tracked by github. Add the files just like you would any other file.\nIf you create a folder named data this will cause problems.\nFor larger datasets, you’ll need to create a new folder in the project root directory named dataset-ignore. This will be ignored by git (based off the .gitignore file in the project root directory) which will help you avoid issues with Github’s size limits. Your team will have to manually make sure the data files in dataset-ignore are synced across team members.\nYour load_and_clean_data.R file is how you will load and clean your data.\n\n# run this to load the dataset from the rds\ndata &lt;- readRDS(file = \"dataset/cleaned_h1b_data.rds\")\n# your code here\nhead(data)\n\n       Country Continent FY97 FY98 FY99 FY00 FY01 FY02 FY03 FY04 FY05 FY06 FY07\n1      Algeria    Africa   52   49   53   64   75   43   17   36   28   21   31\n2       Angola    Africa    0    0    2    2    2    4    6    8    5   10   10\n3        Benin    Africa   10    4    5    8   13    8    7   13    7    6    8\n4     Botswana    Africa    2    5    5    3    9    8   11   21   12   16   13\n5 Burkina Faso    Africa    3    6    8    4    6   13   16   14   12   16   16\n6      Burundi    Africa    1    1    1    1    0    0    1    0    2    1    2\n  FY08 FY09 FY10 FY11 FY12 FY13 FY14 FY15 FY16 FY17 FY18 FY19 FY20 FY21 FY22\n1   21   12   15   17    8   14   22   10   18   12   11   12    4    3   18\n2    7    5   12    2    6    8    5    3    6    5    4    3    1    1    9\n3    9    8   10    5   10    4    5    7    9    9    5    8    4    1    3\n4   14    7    7   13   14    6    5    8   12    2    5    9    3    6   14\n5   13   19   15   12   11   12    7    7    6   11    6    7    1    8    9\n6    2    6    4    2    2    7    4    4    1    0    2    3    2    1    3\n\n\nYou should never use absolute paths (eg. /Users/danielsussman/path/to/project/ or C:\\MA415\\\\Final_Project\\).\nYou might consider using the here function from the here package to avoid path problems.\n\n\nLoad and clean data script\nThe idea behind this file is that someone coming to your website could largely replicate your analyses after running this script on the original data sets to clean them. This file might create a derivative data set that you then use for your subsequent analysis. Note that you don’t need to run this script from every post/page. Instead, you can load in the results of this script, which could be plain text files or .RData files. In your data page you’ll describe how these results were created. If you have a very large data set, you might save smaller data sets that you can use for exploration purposes. To link to this file, you can use [cleaning script](/scripts/load_and_clean_data.R) wich appears as cleaning script."
  },
  {
    "objectID": "data.html#rubric-on-this-page",
    "href": "data.html#rubric-on-this-page",
    "title": "Data",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nDescribe where/how to find data.\n\nYou must include a link to the original data source(s). Make sure to provide attribution to those who collected the data.\nWhy was the data collected/curated? Who put it together? (This is important, if you don’t know why it was collected then that might not be a good dataset to look at.\nDescribe the different data files used and what each variable means.\nIf you have many variables then only describe the most relevant ones and summarize the rest.\n\nDescribe any cleaning you had to do for your data.\n\nYou must include a link to your load_and_clean_data.R file.\nRrename variables and recode factors to make data more clear.\nAlso, describe any additional R packages you used outside of those covered in class.\nDescribe and show code for how you combined multiple data files and any cleaning that was necessary for that.\nSome repetition of what you do in your load_and_clean_data.R file is fine and encouraged if it helps explain what you did.\n\nOrganization, clarity, cleanliness of the page\n\nMake sure to remove excessive warnings, use clean easy-to-read code (without side scrolling), organize with sections, use bullets and other organization tools, etc."
  },
  {
    "objectID": "data.html#first-dataset-background",
    "href": "data.html#first-dataset-background",
    "title": "Data",
    "section": "First Dataset Background",
    "text": "First Dataset Background\nThe original dataset is in dataset directory as .xlsx file. It contains pivot tables for each year that describe various U.S travel approvals such as H1B. To transform the dataset into the current one, we did the following operation\nread in excel files\nfilter out null values(continents, .., etc)\ngroup country\nfilter out other columns and keep H1B column\ncreate columns for each year (1977-2022)\nThe script to transform the dataset is in scripts directory load_and_clean_data.R file.\nThis is an example of how to load our cleaned dataset from dataset directory:\n\n#filename &lt;- file.choose(\"cleaned_h1b_data.rds\")\n#print(filename)\nh1b_country &lt;- \n  read_rds('dataset/cleaned_h1b_data.rds')\n\n\n#changing the year to numerical values\ncurrent_names &lt;- names(h1b_country)\nprint(current_names)\n\n [1] \"Country\"   \"Continent\" \"FY97\"      \"FY98\"      \"FY99\"      \"FY00\"     \n [7] \"FY01\"      \"FY02\"      \"FY03\"      \"FY04\"      \"FY05\"      \"FY06\"     \n[13] \"FY07\"      \"FY08\"      \"FY09\"      \"FY10\"      \"FY11\"      \"FY12\"     \n[19] \"FY13\"      \"FY14\"      \"FY15\"      \"FY16\"      \"FY17\"      \"FY18\"     \n[25] \"FY19\"      \"FY20\"      \"FY21\"      \"FY22\"     \n\n#names(df) &lt;- new_names\n\nThe code below is to rename the columns to understandable years format.\n\ncolnames(h1b_country) &lt;- c('Country','Continent','1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022') \n\nhead(h1b_country)\n\n# A tibble: 6 × 28\n  Country      Continent `1997` `1998` `1999` `2000` `2001` `2002` `2003` `2004`\n  &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Algeria      Africa        52     49     53     64     75     43     17     36\n2 Angola       Africa         0      0      2      2      2      4      6      8\n3 Benin        Africa        10      4      5      8     13      8      7     13\n4 Botswana     Africa         2      5      5      3      9      8     11     21\n5 Burkina Faso Africa         3      6      8      4      6     13     16     14\n6 Burundi      Africa         1      1      1      1      0      0      1      0\n# ℹ 18 more variables: `2005` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;,\n#   `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;,\n#   `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n\nThe following code shortens the dimension of our dataframe to 3 columns specially the years columns. Having many years column can make the dataset hard to analysis or explore.\n\nh1b_continent &lt;- h1b_country |&gt;\n  group_by(Continent)|&gt;\n  select(-Country)|&gt;\n  summarize(across(everything(), sum, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarize()`.\nℹ In argument: `across(everything(), sum, na.rm = TRUE)`.\nℹ In group 1: `Continent = \"Africa\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\ndata_long &lt;- pivot_longer(h1b_continent, cols = -Continent, names_to = \"year\", values_to = \"count_h1b\")\n\nhead(data_long)\n\n# A tibble: 6 × 3\n  Continent year  count_h1b\n  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 Africa    1997       2518\n2 Africa    1998       2634\n3 Africa    1999       2835\n4 Africa    2000       3126\n5 Africa    2001       3808\n6 Africa    2002       2903\n\n\n\n# Sample data manipulation assuming h1b_country is your dataset\nh1b_country_temp &lt;- h1b_country %&gt;%\n  select(-Continent)\n\n# Convert the data from wide to long format\ndata_long_all &lt;- pivot_longer(h1b_country_temp, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\n# Filter for only recent years 2022, 2021, 2020, etc.\nyears_of_interest &lt;- c(\"2022\", \"2021\", \"2020\", \"2019\", \"2018\")\ndata_filtered &lt;- data_long_all %&gt;%\n  filter(year %in% years_of_interest)\n\n# Initialize an empty list to store the pie charts\npiecharts_all &lt;- list()\n\n# Loop only over the filtered years\nfor (year_iter in years_of_interest) {\n  print(year_iter)\n  all_temp &lt;- data_filtered %&gt;% \n    filter(year == year_iter)\n  \n  # Select the top 5 countries based on the count\n  top_countries &lt;- all_temp %&gt;%\n    group_by(Country) %&gt;%\n    summarise(total_count = sum(count_h1b), .groups = \"drop\") %&gt;%\n    arrange(desc(total_count)) %&gt;%\n    slice_max(order_by = total_count, n = 5)  # Updated to use slice_max which is more explicit\n  \n  # Summarise the rest as 'Other'\n  other_countries &lt;- all_temp %&gt;%\n    anti_join(top_countries, by = \"Country\") %&gt;%\n    filter(!is.na(count_h1b)) %&gt;%\n    summarise(Country = \"Other\",\n              total_count = sum(count_h1b), .groups = \"drop\")\n  \n  # Combine the top countries and 'Other'\n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  # Create pie chart\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  # Store the pie chart in the list\n  piecharts_all[[year_iter]] &lt;- pie_chart\n}\n\n[1] \"2022\"\n\n\n\n\n\n[1] \"2021\"\n\n\n\n\n\n[1] \"2020\"\n\n\n\n\n\n[1] \"2019\"\n\n\n\n\n\n[1] \"2018\"\n\n\n\n\n# Print all pie charts stored in the list\nprint(piecharts_all)\n\nlist()\n\n\n\nggplot(data_long, aes(x = year, y = count_h1b, fill = Continent)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  labs(title = \"Total Values by Continent Over the Years\",\n       x = \"Year\", y = \"Total Value\") + theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nggplot(data_long, aes(x = year, y = count_h1b, fill = Continent)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  facet_wrap(~Continent, scales='free_y') + \n  labs(title = \"Total Values by Continent Over the Years\",\n       x = \"Year\", y = \"Total Value\") + theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_x_discrete(guide=guide_axis(check.overlap=TRUE))\n\n\n\n\n\n# Rename year columns\nyears &lt;- paste0(1997:2022)\ncolnames(h1b_country) &lt;- c('Country', 'Continent', years)\n\n# Summarize total H1B visas per year and transform into long format for plotting\ntotal_h1b_by_year_long &lt;- h1b_country %&gt;%\n  select(all_of(years)) %&gt;%\n  summarise(across(everything(), sum, na.rm = TRUE)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"year\", values_to = \"Total_H1B_Visas\")\n\n# Plot total H1B visas issued worldwide over the years\nggplot(total_h1b_by_year_long, aes(x = year, y = Total_H1B_Visas, fill = year)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Total H1B Visas Issued Worldwide Over the Years\",\n       x = \"Year\", y = \"Total Number of H1B Visas\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1), legend.position = \"none\")\n\n\n\n\n\n# Summarize data by continent and transform into long format\ndata_long &lt;- h1b_country %&gt;%\n  group_by(Continent) %&gt;%\n  summarise(across(all_of(years), sum, na.rm = TRUE)) %&gt;%\n  pivot_longer(cols = all_of(years), names_to = \"year\", values_to = \"count_h1b\")\n\n# Plot total values by continent over the years\nggplot(data_long, aes(x = year, y = count_h1b, fill = Continent)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~Continent, scales=\"free_y\") +\n  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) +\n  labs(title = \"Total H1B Visas by Continent Over the Years\",\n       x = \"Year\", y = \"Total Visas\") +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.margin = unit(c(1, 1, 1, 1), \"cm\"))"
  },
  {
    "objectID": "data.html#explaination",
    "href": "data.html#explaination",
    "title": "Data",
    "section": "Explaination",
    "text": "Explaination\nWhile this dataset tells us about the country of origin of visa recipients for H1B-type visas. We believe that dataset 3 allows us to have a deeper understanding of the recent years and employment"
  },
  {
    "objectID": "data.html#background",
    "href": "data.html#background",
    "title": "Data",
    "section": "Background",
    "text": "Background\nSince there exist many columns, we will list a few important columns.\nCASE_NUMBER: Unique identifier assigned to each application submitted for processing to OFLC.\nCASE_STATUS: decision on application. Once certified, employer can proceed with next steps of the visa process to get VISAs for foreign worker\nFULL_TIME_POSITION: Y for getting fulltime and N for not getting\nTOTAL_WORKER_POSITIONS: Total number of foreign workers requested by employers\nEMPLOYER_NAME: Business name\nPREVAILING_WAGE: average wage based on labor market\nWAGE_RATE_OF_PAY_FROM: wage paid at the site\nWORKSITE_CITY: City of the worksite\nAGENT_REPRESENTING_EMPLOYER: employer represented by agent or attorney, Y or N\nRECEIVED_DATE: Date the application received by OFLC\nSource of Dataset: U.S Department Labor\nWe downloaded LCA Programs H-1B and PERM disclosure data\nThe data is made available as part of a broader government initiative toward transparency and open governance. This initiative aims to make government operations more accessible and understandable to the public.\nBy providing access to historical and current data about immigration-related labor applications, the dataset aids in understand trends, evaluate the effectiveness of current policies, and plan future actions based on solid data.\nA couple of ideas we can draw from this dataset are…\n\nprovide insights into the labor market\nimmigration trends\nimpacts on the economy\n\nThe data includes information like case numbers, decision dates, and other metadata which are extracted from applications processed by the OFLC(Office of Foreign Labor Certification). This also includes newly added fields from updated visa application forms and systems, making the data more comprehensive over time. However, this dataset did not include personal or other case details due to privacy and integrity.\nOverall, this dataset is a critical resource for enhancing understanding and facilitating a data-driven approach to managing and legislating foreign labor and immigration-related activities."
  },
  {
    "objectID": "data.html#how-it-was-collected",
    "href": "data.html#how-it-was-collected",
    "title": "Data",
    "section": "How it was collected?",
    "text": "How it was collected?\nRecall that the dataset was provided by OFLC(Office of Foreign Labor Certification).\nEmployers who are seeking to hire foreign workers must submit various forms of applications for prevailing wage determinations, labor certifications, and labor attestations. These applications are required for different visa programs, such as H-1B, H-2A, H-2B, and permanent residency applications. Our goal is focus on H-1B but not limited to explore other programs."
  },
  {
    "objectID": "data.html#why-it-is-worth-exploring",
    "href": "data.html#why-it-is-worth-exploring",
    "title": "Data",
    "section": "Why it is worth exploring?",
    "text": "Why it is worth exploring?\nBecause our groups are interested in the labor market in U.S specifically in foreign labors. This dataset allows us to analyze the labor market dynamics related to foreign labor in the U.S. You can track trends in wages, understand the demand for labor in specific sectors, and assess how immigration policies impact the economy. This kind of analysis is crucial for crafting informed labor and economic policies."
  },
  {
    "objectID": "data.html#our-contribution",
    "href": "data.html#our-contribution",
    "title": "Data",
    "section": "Our contribution",
    "text": "Our contribution\nHere, we will look into two related datasets, both from the same source.\nPERM_disclosure_data: A system used by the U.S Department of Labor to process labor certifications, which is the first step required in the process for certain foreign nationals to obtain an employment-based immigrant visa, or green card. The PERM disclosure data typically includes information about applications submitted by employers who wish to hire foreign workers permanently. This data can provide insights into which companies are applying for labor certifications, the positions they are looking to fill, the salaries offered, the status of applications (whether they were certified, denied, or withdrawn), and other relevant details.\nLCA_Program_data: LCA stands for Labor Condition Application. This application is a component of the H1B visa process, where employers in the U.S. must attest to the Department of Labor that they will pay the H1B visa holder a wage that is no less than the wage paid to similarly qualified U.S. workers. It also ensures that employing a foreign worker will not adversely affect the working conditions of U.S. workers similarly employed. The LCA Program H1B data contains information regarding these applications, including details about the employers filing them, the positions for which they are filed, wage levels, locations of employment, and the status of these applications.\nOur direction is to start initial progress on LCA_Program_data to testify if H1B visa holder has a wage similar to qualified U.S. workers. Then, we will shift to perm_disclosure_data which is the where the main analysis is done."
  },
  {
    "objectID": "data.html#cleaning-dataset",
    "href": "data.html#cleaning-dataset",
    "title": "Data",
    "section": "Cleaning Dataset",
    "text": "Cleaning Dataset\nThe following steps would be describing how we load our dataset and merge additional columns.\n\ndf &lt;- readRDS(\"dataset/cleaned_disclosure.rds\")\nhead(df)\n\n# A tibble: 6 × 12\n  CASE_NUMBER     CASE_STATUS RECEIVED_DATE       DECISION_DATE       VISA_CLASS\n  &lt;chr&gt;           &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;     \n1 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n2 I-203-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 E-3 Austr…\n3 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n4 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n5 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n6 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n# ℹ 7 more variables: JOB_TITLE &lt;chr&gt;, FULL_TIME_POSITION &lt;chr&gt;,\n#   BEGIN_DATE &lt;dttm&gt;, END_DATE &lt;dttm&gt;, EMPLOYER_NAME &lt;chr&gt;,\n#   EMPLOYER_STATE &lt;chr&gt;, AGENT_REPRESENTING_EMPLOYER &lt;chr&gt;\n\n\nWe selected this dataset, updated in 2024, because it comprehensively represents the evolution of U.S. immigration policy from the Trump to the Biden administration. It provides a wealth of columns suitable for statistical modeling. To enrich our analysis, we plan to merge it with two other datasets. The first includes information on the applicants’ universities and majors, while the second contains data on their incomes, allowing us to construct a linear regression model. Once merged, we will have a dataframe with approximately 30+ columns.\nThis is the line of code of how to read xlsx files and convert into rds. We commented out after done. Afterward, we can load in rds files and merge the columns\n\n# rds for the other two projects\n# df2 &lt;- read_excel(\"dataset/worksite.xlsx\")\n# saveRDS(df2, \"dataset/LCA_worksite.rds\")\n\n# df3 &lt;- read_excel(\"dataset/appendix.xlsx\")\n# saveRDS(df3, \"dataset/LCA_appendix.rds\")\n\n\n# read in df2\n\ndf2 &lt;- readRDS(\"dataset/LCA_worksite.rds\")\nhead(df2)\n\n# A tibble: 6 × 22\n  CASE_NUMBER        WORKSITE_WORKERS SECONDARY_ENTITY SECONDARY_ENTITY_BUSINE…¹\n  &lt;chr&gt;                         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;                    \n1 I-200-19297-106681                1 N                &lt;NA&gt;                     \n2 I-200-19319-148860                1 N                &lt;NA&gt;                     \n3 I-200-19319-148860                1 N                &lt;NA&gt;                     \n4 I-200-19319-148860                1 Y                Acuity Eyecare Group     \n5 I-200-19319-149308                1 N                &lt;NA&gt;                     \n6 I-200-19323-154281                1 N                &lt;NA&gt;                     \n# ℹ abbreviated name: ¹​SECONDARY_ENTITY_BUSINESS_NAME\n# ℹ 18 more variables: WORKSITE_ADDRESS1 &lt;chr&gt;, WORKSITE_ADDRESS2 &lt;chr&gt;,\n#   WORKSITE_CITY &lt;chr&gt;, WORKSITE_COUNTY &lt;chr&gt;, WORKSITE_STATE &lt;chr&gt;,\n#   WORKSITE_POSTAL_CODE &lt;dbl&gt;, WAGE_RATE_OF_PAY_FROM &lt;dbl&gt;,\n#   WAGE_RATE_OF_PAY_TO &lt;dbl&gt;, WAGE_UNIT_OF_PAY &lt;chr&gt;, PREVAILING_WAGE &lt;dbl&gt;,\n#   PW_UNIT_OF_PAY &lt;chr&gt;, PW_TRACKING_NUMBER &lt;chr&gt;, PW_WAGE_LEVEL &lt;chr&gt;,\n#   PW_OES_YEAR &lt;chr&gt;, PW_OTHER_SOURCE &lt;chr&gt;, PW_OTHER_YEAR &lt;dbl&gt;, …\n\n\n\n# read in df3\n\ndf3 &lt;- readRDS(\"dataset/appendix.rds\")\nhead(df3)\n\n# A tibble: 6 × 5\n  CASE_NUMBER        APPX_A_NO_OF_EXEMPT_WORKERS APPX_A_NAME_OF_INSTITUTION     \n  &lt;chr&gt;                                    &lt;dbl&gt; &lt;chr&gt;                          \n1 I-200-20353-968632                           1 The William Paterson Universit…\n2 I-200-21116-258512                           1 GOVERNORS STATE UNIVERISTY     \n3 I-200-22115-102220                           1 Lamar University               \n4 I-200-22126-145447                           1 California State University, L…\n5 I-200-22164-272276                           1 Illinois Institute of Technolo…\n6 I-200-22180-320955                           1 University of Missouri         \n# ℹ 2 more variables: APPX_A_FIELD_OF_STUDY &lt;chr&gt;, APPX_A_DATE_OF_DEGREE &lt;dttm&gt;\n\n\nTo merge the dataset together, we can find matching case numbers. The additional dataset gives additional information about applicants university, field of major, date of degree etc…\nHere we commented it out since the dataset has already been merged.\n\n# merge df1 and df2 by case number\n# merge_1 &lt;- merge(df, df2, by=\"CASE_NUMBER\", all = TRUE)\n\n\n# final_merge &lt;- merge(merge_1, df3, by=\"CASE_NUMBER\", all = TRUE)\n\n\n# save as new rds for final_merge\n# saveRDS(final_merge, \"dataset/final_merge.rds\")"
  },
  {
    "objectID": "data.html#eda-version-2",
    "href": "data.html#eda-version-2",
    "title": "Data",
    "section": "EDA version 2",
    "text": "EDA version 2\n\nfinal_df &lt;- readRDS(\"dataset/final_merge.rds\")\nhead(final_df)\n\n         CASE_NUMBER           CASE_STATUS RECEIVED_DATE DECISION_DATE\n1 I-200-19297-106681 Certified - Withdrawn    2019-10-24    2023-11-01\n2 I-200-19319-148860 Certified - Withdrawn    2019-11-15    2023-10-20\n3 I-200-19319-148860 Certified - Withdrawn    2019-11-15    2023-10-20\n4 I-200-19319-148860 Certified - Withdrawn    2019-11-15    2023-10-20\n5 I-200-19319-149308 Certified - Withdrawn    2019-11-15    2023-10-05\n6 I-200-19323-154281 Certified - Withdrawn    2019-11-19    2023-10-02\n  VISA_CLASS                JOB_TITLE FULL_TIME_POSITION BEGIN_DATE   END_DATE\n1       H-1B     Research Scientist I                  Y 2019-10-24 2022-10-23\n2       H-1B       Software Developer                  Y 2019-12-01 2022-11-30\n3       H-1B       Software Developer                  Y 2019-12-01 2022-11-30\n4       H-1B       Software Developer                  Y 2019-12-01 2022-11-30\n5       H-1B       Software Developer                  Y 2019-11-18 2022-11-17\n6       H-1B SENIOR SOFTWARE ENGINEER                  Y 2020-05-16 2023-05-15\n                       EMPLOYER_NAME EMPLOYER_STATE AGENT_REPRESENTING_EMPLOYER\n1    Georgia Institute of Technology             GA                          No\n2                     TRISHULLA, LLC             WI                         Yes\n3                     TRISHULLA, LLC             WI                         Yes\n4                     TRISHULLA, LLC             WI                         Yes\n5 INTERNATIONAL SOFTWARE SYSTEMS INC             MD                          No\n6            UBER TECHNOLOGIES, INC.             CA                         Yes\n  WORKSITE_WORKERS SECONDARY_ENTITY SECONDARY_ENTITY_BUSINESS_NAME\n1                1                N                           &lt;NA&gt;\n2                1                Y           Acuity Eyecare Group\n3                1                N                           &lt;NA&gt;\n4                1                N                           &lt;NA&gt;\n5                1                N                           &lt;NA&gt;\n6                1                N                           &lt;NA&gt;\n                     WORKSITE_ADDRESS1 WORKSITE_ADDRESS2 WORKSITE_CITY\n1                      313 Ferst Drive              &lt;NA&gt;       Atlanta\n2 4835 Lyndon B Johnson Fwy, Suite 850              &lt;NA&gt;        Dallas\n3       6629 UNIVERSITY AVE, SUITE 210              &lt;NA&gt;     MIDDLETON\n4       8383 Greenway Blvd., Suite 600              &lt;NA&gt;     Middleton\n5                 7337 Hanover Parkway           Suite A     Greenbelt\n6                   1455 MARKET STREET         4TH FLOOR SAN FRANCISCO\n  WORKSITE_COUNTY WORKSITE_STATE WORKSITE_POSTAL_CODE WAGE_RATE_OF_PAY_FROM\n1          FULTON        GEORGIA                30332                 52100\n2          DALLAS          TEXAS                75244                 77605\n3            DANE      WISCONSIN                53562                 77605\n4            DANE      WISCONSIN                53562                 77605\n5 PRINCE GEORGE'S       MARYLAND                20770                 76000\n6   SAN FRANCISCO     CALIFORNIA                94103                202442\n  WAGE_RATE_OF_PAY_TO WAGE_UNIT_OF_PAY PREVAILING_WAGE PW_UNIT_OF_PAY\n1                  NA             Year           46821           Year\n2               77700             Year           77605           Year\n3               77700             Year           65458           Year\n4               77700             Year           65458           Year\n5                  NA             Year           75712           Year\n6                  NA             Year          168958           Year\n  PW_TRACKING_NUMBER PW_WAGE_LEVEL          PW_OES_YEAR PW_OTHER_SOURCE\n1               &lt;NA&gt;             I 7/1/2019 - 6/30/2020            &lt;NA&gt;\n2               &lt;NA&gt;             I 7/1/2019 - 6/30/2020            &lt;NA&gt;\n3               &lt;NA&gt;             I 7/1/2019 - 6/30/2020            &lt;NA&gt;\n4               &lt;NA&gt;             I 7/1/2019 - 6/30/2020            &lt;NA&gt;\n5               &lt;NA&gt;             I 7/1/2019 - 6/30/2020            &lt;NA&gt;\n6               &lt;NA&gt;            IV 7/1/2019 - 6/30/2020            &lt;NA&gt;\n  PW_OTHER_YEAR PW_SURVEY_PUBLISHER PW_SURVEY_NAME APPX_A_NO_OF_EXEMPT_WORKERS\n1            NA                &lt;NA&gt;           &lt;NA&gt;                          NA\n2            NA                &lt;NA&gt;           &lt;NA&gt;                          NA\n3            NA                &lt;NA&gt;           &lt;NA&gt;                          NA\n4            NA                &lt;NA&gt;           &lt;NA&gt;                          NA\n5            NA                &lt;NA&gt;           &lt;NA&gt;                          NA\n6            NA                &lt;NA&gt;           &lt;NA&gt;                          NA\n  APPX_A_NAME_OF_INSTITUTION APPX_A_FIELD_OF_STUDY APPX_A_DATE_OF_DEGREE\n1                       &lt;NA&gt;                  &lt;NA&gt;                  &lt;NA&gt;\n2                       &lt;NA&gt;                  &lt;NA&gt;                  &lt;NA&gt;\n3                       &lt;NA&gt;                  &lt;NA&gt;                  &lt;NA&gt;\n4                       &lt;NA&gt;                  &lt;NA&gt;                  &lt;NA&gt;\n5                       &lt;NA&gt;                  &lt;NA&gt;                  &lt;NA&gt;\n6                       &lt;NA&gt;                  &lt;NA&gt;                  &lt;NA&gt;\n\n\n\n# filter the dataset which jobs have most frequency and applicant get full time position\nfiltered_data &lt;- final_df |&gt;\n  filter(FULL_TIME_POSITION == \"Y\") |&gt;\n  count(JOB_TITLE, sort = TRUE) |&gt;\n  top_n(5)\n\nSelecting by n\n\nprint(filtered_data)\n\n                 JOB_TITLE    n\n1        Software Engineer 8481\n2       Software Developer 5442\n3 Senior Software Engineer 2260\n4       SOFTWARE DEVELOPER 1723\n5      Assistant Professor 1183\n\n\n\n# extract year, month, day from date\nfinal_df$YEAR_RECEIVED = as.integer(substr(final_df$RECEIVED_DATE, 1, 4))\nfinal_df$MONTH_RECEIVED = month(final_df$RECEIVED_DATE)\nfinal_df$DAY_RECEIVED = day(final_df$RECEIVED_DATE)\n\n\nfinal_df$YEAR_END = as.integer(substr(final_df$END_DATE, 1, 4))\nfinal_df$MONTH_END = month(final_df$END_DATE)\nfinal_df$DAY_END = day(final_df$END_DATE)\n\nfinal_df$YEAR_DECISION = as.integer(substr(final_df$DECISION_DATE, 1, 4))\nfinal_df$MONTH_DECISION = month(final_df$DECISION_DATE)\nfinal_df$DAY_DECISION = day(final_df$DECISION_DATE)\n\n\nfinal_df$YEAR_BEGIN = as.integer(substr(final_df$BEGIN_DATE, 1, 4))\nfinal_df$MONTH_BEGIN = month(final_df$BEGIN_DATE)\nfinal_df$DAY_BEGIN = day(final_df$BEGIN_DATE)\n\n\n# filter the dataset which jobs have most frequency and applicant get full time position\nfiltered_data &lt;- final_df |&gt;\n  count(EMPLOYER_NAME, sort = TRUE) |&gt;\n  top_n(5)\n\nSelecting by n\n\nprint(filtered_data)\n\n                           EMPLOYER_NAME    n\n1                Amazon.com Services LLC 3259\n2                 Ernst & Young U.S. LLP 2610\n3                             Google LLC 2325\n4                  Microsoft Corporation 2190\n5 COGNIZANT TECHNOLOGY SOLUTIONS US CORP 1639\n\n\n\nfiltered_df &lt;- final_df %&gt;%\n  # filter(JOB_TITLE %in% c(\"Software Engineer\", \"Assistant Professor\", \"Business Analyst\", \"Machine Learning Engineer\"),\n  # FULL_TIME_POSITION %in% c(\"Y\", \"N\")) %&gt;%\n  select(WAGE_RATE_OF_PAY_FROM, PREVAILING_WAGE)\n\nhead(filtered_df)\n\n  WAGE_RATE_OF_PAY_FROM PREVAILING_WAGE\n1                 52100           46821\n2                 77605           77605\n3                 77605           65458\n4                 77605           65458\n5                 76000           75712\n6                202442          168958\n\n\nNext, we will plot the log scale of prevailing_wage and wage_rate_of_pay_from.\n\n# Assuming final_df is already loaded and available\n\nfiltered_df &lt;- final_df %&gt;%\n  filter(\n         FULL_TIME_POSITION %in% c(\"Y\", \"N\"),\n         # some wage unit of pay is in hours.\n         WAGE_UNIT_OF_PAY %in% c(\"Year\")) |&gt; \n  select(WAGE_RATE_OF_PAY_FROM, PREVAILING_WAGE, WAGE_UNIT_OF_PAY)\n\n# Create a ggplot scatter plot with a linear regression line\nggplot(filtered_df, aes(x = log(PREVAILING_WAGE), y = log(WAGE_RATE_OF_PAY_FROM) )) +\n  geom_point(alpha = 0.5) +  # Adds scatter plot points with some transparency\n  geom_smooth(method = \"lm\", se = TRUE) +  # Adds a linear regression line without the confidence interval\n  labs(x = \"Prevailing Wage\", y = \"Wage Rate of Pay From\", title = \"Relationship between Prevailing Wage and Wage Rate of Pay\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe can see there there exist linear relationship between Prevailing wage and wage paid at worksite. To recall, prevailing wage is the average wage paid according to the labor market. It would be a surprise if foreign workers are paid less than their peers.\n\n# Identify states with high number of applicants or high approval rates\nstate_df &lt;- final_df\n\nstate_df &lt;- na.omit(state_df[c(\"EMPLOYER_STATE\", \"CASE_STATUS\")])\n\nstate_data &lt;- state_df |&gt;\n  group_by(EMPLOYER_STATE) |&gt;\n  summarise(\n    Total_Applications = n(),\n    Approved_Applications = sum(CASE_STATUS == \"Approved\")\n  )\n\n\n# visualizing\n# Sort by total applications\nstate_data_sorted_applicants &lt;- state_data %&gt;%\n  arrange(desc(Total_Applications))\n\ntop_states &lt;- state_data_sorted_applicants |&gt;\n  top_n(20, Total_Applications)\n\n# Plot for Total Applications\nggplot(top_states, aes(x = reorder(EMPLOYER_STATE, -Total_Applications), y = Total_Applications)) +\n  geom_bar(stat = \"identity\", fill = \"GREY\") +\n  theme(axis.text.y = element_text(size = 6)) +  # Smaller text size for states\n  labs(title = \"Total Visa Applications by State\", x = \"State\", y = \"Number of Applications\") +\n  coord_flip()  # Flips the coordinates to make labels readable\n\n\n\n\nTop 20 states where received the highest number of visa applications. In other word, these are the states that hire and submit applications for foreign workers the most (up to date 2024). From previous exploration, we learned that majority of the job titles are tech related jobs, and majority of tech companies still hiring located in these states.\n\nyearly_wage &lt;- final_df %&gt;%\n  group_by(YEAR_BEGIN) %&gt;%\n  summarise(Avg_Prevailing_Wage = mean(PREVAILING_WAGE, na.rm = TRUE))\n\nggplot(yearly_wage, aes(x = YEAR_BEGIN, y = Avg_Prevailing_Wage)) +\n  geom_line() +  # Use a line plot to show trends over time\n  geom_point() +  # Add points to highlight each year's average wage\n  labs(title = \"Average Prevailing Wage Per Year\",\n       x = \"Year\",\n       y = \"Average Prevailing Wage\") \n\n\n\n\nAverage prevailing wage might not be a good choice since tech jobs would skew the average. We would take a look at the medians.\n\nyearly_wage &lt;- final_df %&gt;%\n  group_by(YEAR_BEGIN) %&gt;%\n  summarise(median_Prevailing_Wage = median(PREVAILING_WAGE, na.rm = TRUE))\n\nggplot(yearly_wage, aes(x = YEAR_BEGIN, y = median_Prevailing_Wage)) +\n  geom_line() +  # Use a line plot to show trends over time\n  geom_point() +  # Add points to highlight each year's average wage\n  labs(title = \"Median Prevailing Wage Per Year\",\n       x = \"Year\",\n       y = \"Median Prevailing Wage\") \n\n\n\n\nBiden becomes president at year 2020. A sudden drop in wages happen in year 2021 due to covid19. Afterward, the trends goes right back up."
  },
  {
    "objectID": "data.html#perm_disclosure_data",
    "href": "data.html#perm_disclosure_data",
    "title": "Data",
    "section": "PERM_DISCLOSURE_DATA",
    "text": "PERM_DISCLOSURE_DATA\nTo recall, the first step required in the process for certain foreign nationals to obtain an employment-based immigrant visa, or green card is in PERM_DISCLOSURE_DATA It contains information about applications submitted by employers who wish to hire foreign workers permanently. This data can provide insights into which companies are applying for labor certifications, the positions they are looking to fill, the salaries offered, the status of applications (whether they were certified, denied, or withdrawn), and other relevant details.\nHere, we will load in the data and do basic inspection of the data. Acknowledging that this dataset contains many columns. We can create a separate df that contains columns we believe is important.\nNote that PERM_DISCLOSURE_DATA is almost the identical as LCA_Program data but PERM_DISCLOSURE_DATA contains more columns which you can select various columns depend on your needs.\nCommented it out since this is one time operation\n\n# perm_data &lt;- read_excel(\"dataset/PERM_data.xlsx\")\n# saveRDS(perm_data, \"dataset/perm_data.rds\")\n\n\nperm_df &lt;- readRDS(\"dataset/perm_data.rds\")\nhead(perm_df)\n\n# A tibble: 6 × 154\n  CASE_NUMBER   CASE_STATUS RECEIVED_DATE       DECISION_DATE       REFILE\n  &lt;chr&gt;         &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt; \n1 A-23043-00641 Withdrawn   2023-02-12 00:00:00 2023-10-01 00:00:00 N     \n2 A-22256-20380 Certified   2022-09-22 00:00:00 2023-10-02 00:00:00 N     \n3 A-22271-30061 Certified   2022-10-24 00:00:00 2023-10-02 00:00:00 N     \n4 A-22272-30740 Certified   2022-10-24 00:00:00 2023-10-02 00:00:00 N     \n5 A-22185-82932 Certified   2022-07-26 00:00:00 2023-10-02 00:00:00 N     \n6 A-22189-85099 Certified   2022-11-16 00:00:00 2023-10-02 00:00:00 N     \n# ℹ 149 more variables: ORIG_FILE_DATE &lt;dttm&gt;,\n#   PREVIOUS_SWA_CASE_NUMBER_STATE &lt;chr&gt;, SCHD_A_SHEEPHERDER &lt;chr&gt;,\n#   EMPLOYER_NAME &lt;chr&gt;, EMPLOYER_ADDRESS_1 &lt;chr&gt;, EMPLOYER_ADDRESS_2 &lt;chr&gt;,\n#   EMPLOYER_CITY &lt;chr&gt;, EMPLOYER_STATE_PROVINCE &lt;chr&gt;, EMPLOYER_COUNTRY &lt;chr&gt;,\n#   EMPLOYER_POSTAL_CODE &lt;dbl&gt;, EMPLOYER_PHONE &lt;chr&gt;, EMPLOYER_PHONE_EXT &lt;chr&gt;,\n#   EMPLOYER_NUM_EMPLOYEES &lt;dbl&gt;, EMPLOYER_YEAR_COMMENCED_BUSINESS &lt;dbl&gt;,\n#   NAICS_CODE &lt;dbl&gt;, FW_OWNERSHIP_INTEREST &lt;chr&gt;, EMP_CONTACT_NAME &lt;chr&gt;, …\n\n\n\n# let's check missing values\nprint(colSums(is.na(perm_df)))\n\n                       CASE_NUMBER                        CASE_STATUS \n                                 0                                  0 \n                     RECEIVED_DATE                      DECISION_DATE \n                                 0                                  0 \n                            REFILE                     ORIG_FILE_DATE \n                                 0                              24953 \n    PREVIOUS_SWA_CASE_NUMBER_STATE                 SCHD_A_SHEEPHERDER \n                             24934                                  0 \n                     EMPLOYER_NAME                 EMPLOYER_ADDRESS_1 \n                                 0                                  0 \n                EMPLOYER_ADDRESS_2                      EMPLOYER_CITY \n                             17503                                  0 \n           EMPLOYER_STATE_PROVINCE                   EMPLOYER_COUNTRY \n                                 1                                  9 \n              EMPLOYER_POSTAL_CODE                     EMPLOYER_PHONE \n                               112                                  0 \n                EMPLOYER_PHONE_EXT             EMPLOYER_NUM_EMPLOYEES \n                             23505                                  3 \n  EMPLOYER_YEAR_COMMENCED_BUSINESS                         NAICS_CODE \n                                15                                 23 \n             FW_OWNERSHIP_INTEREST                   EMP_CONTACT_NAME \n                                 1                                  0 \n             EMP_CONTACT_ADDRESS_1              EMP_CONTACT_ADDRESS_2 \n                                 2                              16610 \n                  EMP_CONTACT_CITY         EMP_CONTACT_STATE_PROVINCE \n                                 0                                  0 \n               EMP_CONTACT_COUNTRY            EMP_CONTACT_POSTAL_CODE \n                                 7                                  0 \n                 EMP_CONTACT_PHONE                  EMP_CONTACT_EMAIL \n                                 1                                  4 \n               AGENT_ATTORNEY_NAME           AGENT_ATTORNEY_FIRM_NAME \n                              2014                               2291 \n              AGENT_ATTORNEY_PHONE           AGENT_ATTORNEY_PHONE_EXT \n                              2015                              24415 \n          AGENT_ATTORNEY_ADDRESS_1           AGENT_ATTORNEY_ADDRESS_2 \n                              2015                               7825 \n               AGENT_ATTORNEY_CITY      AGENT_ATTORNEY_STATE_PROVINCE \n                              2013                               2200 \n            AGENT_ATTORNEY_COUNTRY         AGENT_ATTORNEY_POSTAL_CODE \n                              2012                               2033 \n              AGENT_ATTORNEY_EMAIL                    PW_TRACK_NUMBER \n                              2017                                128 \n                       PW_SOC_CODE                       PW_SOC_TITLE \n                                21                                  3 \n                    PW_SKILL_LEVEL                            PW_WAGE \n                               127                                  3 \n                    PW_UNIT_OF_PAY                     PW_WAGE_SOURCE \n                                 3                                  5 \n              PW_SOURCE_NAME_OTHER              PW_DETERMINATION_DATE \n                             24002                                 19 \n                PW_EXPIRATION_DATE                    WAGE_OFFER_FROM \n                                21                                  2 \n                     WAGE_OFFER_TO             WAGE_OFFER_UNIT_OF_PAY \n                             14811                                  1 \n                WORKSITE_ADDRESS_1                 WORKSITE_ADDRESS_2 \n                                 1                              16021 \n                     WORKSITE_CITY                     WORKSITE_STATE \n                                 0                                  2 \n              WORKSITE_POSTAL_CODE                          JOB_TITLE \n                                 1                                  1 \n                 MINIMUM_EDUCATION            JOB_EDUCATION_MIN_OTHER \n                                 5                              24453 \n              MAJOR_FIELD_OF_STUDY                  REQUIRED_TRAINING \n                              5988                                  0 \n          REQUIRED_TRAINING_MONTHS         REQUIRED_FIELD_OF_TRAINING \n                             24775                              24731 \n               REQUIRED_EXPERIENCE         REQUIRED_EXPERIENCE_MONTHS \n                                 0                              11595 \n         ACCEPT_ALT_FIELD_OF_STUDY      ACCEPT_ALT_MAJOR_FLD_OF_STUDY \n                                 1                              13966 \n                  ACCEPT_ALT_COMBO         ACCEPT_ALT_COMBO_EDUCATION \n                                 1                              18459 \n         ACCEPT_ALT_COMBO_ED_OTHER     ACCEPT_ALT_COMBO_EDUCATION_YRS \n                             24651                              18511 \n          ACCEPT_FOREIGN_EDUCATION              ACCEPT_ALT_OCCUPATION \n                                 1                                  2 \n      ACCEPT_ALT_OCCUPATION_MONTHS               ACCEPT_ALT_JOB_TITLE \n                              8402                               8348 \n       JOB_OPP_REQUIREMENTS_NORMAL          FOREIGN_LANGUAGE_REQUIRED \n                                 0                                  0 \n                   SPECIFIC_SKILLS             COMBINATION_OCCUPATION \n                              1841                                  0 \n    OFFERED_TO_APPL_FOREIGN_WORKER        FOREIGN_WORKER_LIVE_ON_PREM \n                                 0                                  0 \n    FOREIGN_WORKER_LIVE_IN_DOM_SER FOREIGN_WORKER_LIVE_IN_DOM_SVC_CNT \n                                 0                              24790 \n           PROFESSIONAL_OCCUPATION          APP_FOR_COLLEGE_U_TEACHER \n                                 0                                  1 \n               COMPETITIVE_PROCESS          BASIC_RECRUITMENT_PROCESS \n                             24774                              24778 \n               TEACHER_SELECT_DATE           TEACHER_PUB_JOURNAL_NAME \n                             24839                              24791 \n           ADD_RECRUIT_INFORMATION           SWA_JOB_ORDER_START_DATE \n                             24713                                153 \n            SWA_JOB_ORDER_END_DATE           SUNDAY_EDITION_NEWSPAPER \n                               154                                138 \n              FIRST_NEWSPAPER_NAME     FIRST_ADVERTISEMENT_START_DATE \n                               140                                151 \n          SECOND_NEWSPAPER_AD_NAME          SECOND_ADVERTISEMENT_TYPE \n                               143                                152 \n              SECOND_AD_START_DATE                 JOB_FAIR_FROM_DATE \n                               152                              24915 \n                  JOB_FAIR_TO_DATE     ON_CAMPUS_RECRUITING_FROM_DATE \n                             24915                              24866 \n      ON_CAMPUS_RECRUITING_TO_DATE         EMPLOYER_WEBSITE_FROM_DATE \n                             24866                              14699 \n          EMPLOYER_WEBSITE_TO_DATE               PRO_ORG_AD_FROM_DATE \n                             14699                              20371 \n     PRO_ORG_ADVERTISEMENT_TO_DATE       JOB_SEARCH_WEBSITE_FROM_DATE \n                             20371                               6877 \n        JOB_SEARCH_WEBSITE_TO_DATE      PVT_EMPLOYMENT_FIRM_FROM_DATE \n                              6877                              24158 \n       PVT_EMPLOYMENT_FIRM_TO_DATE        EMPLOYEE_REF_PROG_FROM_DATE \n                             24158                              20776 \n    EMPLOYEE_REFERRAL_PROG_TO_DATE         CAMPUS_PLACEMENT_FROM_DATE \n                             20776                              23424 \n          CAMPUS_PLACEMENT_TO_DATE       LOCAL_ETHNIC_PAPER_FROM_DATE \n                             23424                              10316 \n        LOCAL_ETHNIC_PAPER_TO_DATE              RADIO_TV_AD_FROM_DATE \n                             10316                              21619 \n               RADIO_TV_AD_TO_DATE               EMP_RECEIVED_PAYMENT \n                             21619                                  3 \n                   PAYMENT_DETAILS            BARGAINING_REP_NOTIFIED \n                             24924                                  3 \n         POSTED_NOTICE_AT_WORKSITE          LAYOFF_IN_PAST_SIX_MONTHS \n                                 3                                  3 \n             US_WORKERS_CONSIDERED             COUNTRY_OF_CITIZENSHIP \n                             23565                                  5 \n      FOREIGN_WORKER_BIRTH_COUNTRY                 CLASS_OF_ADMISSION \n                                 3                               2629 \n          FOREIGN_WORKER_EDUCATION     FOREIGN_WORKER_EDUCATION_OTHER \n                                 5                              24350 \n         FOREIGN_WORKER_INFO_MAJOR         FOREIGN_WORKER_YRS_ED_COMP \n                              4418                               4311 \n         FOREIGN_WORKER_INST_OF_ED       FOREIGN_WORKER_ED_INST_ADD_1 \n                              4281                               4467 \n      FOREIGN_WORKER_ED_INST_ADD_2        FOREIGN_WORKER_ED_INST_CITY \n                             22251                               4318 \n    FOREIGN_WORKER_ED_INST_STATE_P     FOREIGN_WORKER_ED_INST_COUNTRY \n                              6021                               4491 \n    FOREIGN_WORKER_ED_INST_POST_CD       FOREIGN_WORKER_TRAINING_COMP \n                              5224                                  1 \n     FOREIGN_WORKER_REQ_EXPERIENCE          FOREIGN_WORKER_ALT_ED_EXP \n                                 0                                  0 \n        FOREIGN_WORKER_ALT_OCC_EXP       FOREIGN_WORKER_EXP_WITH_EMPL \n                                 1                                  1 \n    FOREIGN_WORKER_EMPL_PAY_FOR_ED       FOREIGN_WORKER_CURR_EMPLOYED \n                                 0                                  0 \n    EMPLOYER_COMPLETED_APPLICATION                      PREPARER_NAME \n                                 1                               2116 \n                    PREPARER_TITLE                     PREPARER_EMAIL \n                              2118                               2121 \n                EMP_INFO_DECL_NAME                     EMP_DECL_TITLE \n                                 2                                  3 \n\n\n\n# 24962 rows, 154 columns\ndim(perm_df)\n\n[1] 24962   154"
  },
  {
    "objectID": "data.html#eda-version-3",
    "href": "data.html#eda-version-3",
    "title": "Data",
    "section": "EDA version 3",
    "text": "EDA version 3\n\nfinal_perm_df &lt;- select(perm_df, \"CASE_NUMBER\", \"CASE_STATUS\", \"RECEIVED_DATE\", \"DECISION_DATE\", \"EMPLOYER_NAME\",\n                        \"EMPLOYER_NUM_EMPLOYEES\", \"AGENT_ATTORNEY_FIRM_NAME\", \"PW_WAGE\", \"PW_SKILL_LEVEL\", \"WORKSITE_STATE\", \"JOB_TITLE\", \"PW_UNIT_OF_PAY\", \"WAGE_OFFER_FROM\", \"WAGE_OFFER_UNIT_OF_PAY\",\n                        , \"MINIMUM_EDUCATION\", \"REQUIRED_EXPERIENCE\", \"COUNTRY_OF_CITIZENSHIP\", \"CLASS_OF_ADMISSION\"\n                        )\n\nhead(final_perm_df)\n\n# A tibble: 6 × 18\n  CASE_NUMBER  CASE_STATUS RECEIVED_DATE       DECISION_DATE       EMPLOYER_NAME\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;        \n1 A-23043-006… Withdrawn   2023-02-12 00:00:00 2023-10-01 00:00:00 LIBERTY CIVI…\n2 A-22256-203… Certified   2022-09-22 00:00:00 2023-10-02 00:00:00 Siser North …\n3 A-22271-300… Certified   2022-10-24 00:00:00 2023-10-02 00:00:00 Blend360 LLC \n4 A-22272-307… Certified   2022-10-24 00:00:00 2023-10-02 00:00:00 Blend360 LLC \n5 A-22185-829… Certified   2022-07-26 00:00:00 2023-10-02 00:00:00 AMDOCS INC.  \n6 A-22189-850… Certified   2022-11-16 00:00:00 2023-10-02 00:00:00 VMWARE, INC. \n# ℹ 13 more variables: EMPLOYER_NUM_EMPLOYEES &lt;dbl&gt;,\n#   AGENT_ATTORNEY_FIRM_NAME &lt;chr&gt;, PW_WAGE &lt;dbl&gt;, PW_SKILL_LEVEL &lt;chr&gt;,\n#   WORKSITE_STATE &lt;chr&gt;, JOB_TITLE &lt;chr&gt;, PW_UNIT_OF_PAY &lt;chr&gt;,\n#   WAGE_OFFER_FROM &lt;dbl&gt;, WAGE_OFFER_UNIT_OF_PAY &lt;chr&gt;,\n#   MINIMUM_EDUCATION &lt;chr&gt;, REQUIRED_EXPERIENCE &lt;chr&gt;,\n#   COUNTRY_OF_CITIZENSHIP &lt;chr&gt;, CLASS_OF_ADMISSION &lt;chr&gt;"
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis",
    "section": "",
    "text": "This comes from the file analysis.qmd.\nWe describe here our detailed data analysis. This page will provide an overview of what questions you addressed, illustrations of relevant aspects of the data with tables and figures, and a statistical model that attempts to answer part of the question. You’ll also reflect on next steps and further analysis.\nThe audience for this page is someone like your class mates, so you can expect that they have some level of statistical and quantitative sophistication and understand ideas like linear and logistic regression, coefficients, confidence intervals, overfitting, etc.\nWhile the exact number of figures and tables will vary and depend on your analysis, you should target around 5 to 6. An overly long analysis could lead to losing points. If you want you can link back to your blog posts or create separate pages with more details.\nThe style of this paper should aim to be that of an academic paper. I don’t expect this to be of publication quality but you should keep that aim in mind. Avoid using “we” too frequently, for example “We also found that …”. Describe your methodology and your findings but don’t describe your whole process."
  },
  {
    "objectID": "analysis.html#note-on-attribution",
    "href": "analysis.html#note-on-attribution",
    "title": "Analysis",
    "section": "Note on Attribution",
    "text": "Note on Attribution\nIn general, you should try to provide links to relevant resources, especially those that helped you. You don’t have to link to every StackOverflow post you used but if there are explainers on aspects of the data or specific models that you found helpful, try to link to those. Also, try to link to other sources that might support (or refute) your analysis. These can just be regular hyperlinks. You don’t need a formal citation.\nIf you are directly quoting from a source, please make that clear. You can show quotes using &gt; like this\n&gt; To be or not to be.\n\nTo be or not to be."
  },
  {
    "objectID": "analysis.html#rubric-on-this-page",
    "href": "analysis.html#rubric-on-this-page",
    "title": "Analysis",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nIntroduce what motivates your Data Analysis (DA)\n\nWhich variables and relationships are you most interested in?\nWhat questions are you interested in answering?\nProvide context for the rest of the page. This will include figures/tables that illustrate aspects of the data of your question.\n\nModeling and Inference\n\nThe page will include some kind of formal statistical model. This could be a linear regression, logistic regression, or another modeling framework.\nExplain the ideas and techniques you used to choose the predictors for your model. (Think about including interaction terms and other transformations of your variables.)\nDescribe the results of your modelling and make sure to give a sense of the uncertainty in your estimates and conclusions.\n\nExplain the flaws and limitations of your analysis\n\nAre there some assumptions that you needed to make that might not hold? Is there other data that would help to answer your questions?\n\nClarity Figures\n\nAre your figures/tables/results easy to read, informative, without problems like overplotting, hard-to-read labels, etc?\nEach figure should provide a key insight. Too many figures or other data summaries can detract from this. (While not a hard limit, around 5 total figures is probably a good target.)\nDefault lm output and plots are typically not acceptable.\n\nClarity of Explanations\n\nHow well do you explain each figure/result?\nDo you provide interpretations that suggest further analysis or explanations for observed phenomenon?\n\nOrganization and cleanliness.\n\nMake sure to remove excessive warnings, hide most or all code, organize with sections or multiple pages, use bullets, etc.\nThis page should be self-contained, i.e. provide a description of the relevant data."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA [46]15 Final Project",
    "section": "",
    "text": "Final Project due May 7, 2024 at 11:59pm.\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nblogpost5\n\n\n\n\n\nThis blog post will build on blogpost4 to merge the 3rd dataset by case numbers\n\n\n\n\n\n\nApr 10, 2024\n\n\nteam3\n\n\n\n\n\n\n  \n\n\n\n\nblog-post-4\n\n\n\n\n\nThis post details blog-post-4\n\n\n\n\n\n\nApr 7, 2024\n\n\nteam3\n\n\n\n\n\n\n  \n\n\n\n\nblog-post-3\n\n\n\n\n\nThis post details basic explore data exploration.\n\n\n\n\n\n\nApr 5, 2024\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 2\n\n\n\n\n\nThis post details specifics of our dataset and our initial processing steps.\n\n\n\n\n\n\nMar 25, 2024\n\n\nT3am-3\n\n\n\n\n\n\n  \n\n\n\n\nBlog Post 1\n\n\n\n\n\nThis post details specifics of our dataset and our initial processing steps.\n\n\n\n\n\n\nMar 4, 2024\n\n\nT3am-3\n\n\n\n\n\n\n  \n\n\n\n\nExamples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\nGetting started\n\n\n\n\n\n\n\n\n\n\nDirections to set up your website and create your first post.\n\n\n\n\n\n\nFeb 23, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\n  \n\n\n\n\ntest_1\n\n\n\n\n\ntest_1 2/23/24\n\n\n\n\n\n\nFeb 23, 2024\n\n\nweiningmai\n\n\n\n\n\n\n  \n\n\n\n\nFirst Team Meeting\n\n\n\n\n\n\n\n\n\n\nThis post details the steps you’ll take for your first team meeting.\n\n\n\n\n\n\nFeb 21, 2024\n\n\nDaniel Sussman\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-03-23-blogpost2/blogpost2.html",
    "href": "posts/2024-03-23-blogpost2/blogpost2.html",
    "title": "Blog Post 2",
    "section": "",
    "text": "Data background\n\nThe data we will analyze is from the U.S Department of State, more specifically the Bureau of Consular Affairs. The Report of the Visa Office is an annual report providing statistical information on immigrant and non-immigrant visa issuances by consular offices, including information visa allocation in numerically limited categories. The main part of this dataset we want to focus on is the country of origin of visa recipients for H1B-type visas. Years covered range from Fiscal Year 1997 to F.Y. 2022 inclusively.\nRegarding potential biases and issues, the data includes all types of visa applicants processed by U.S. consular offices, which could cause biases such as those related to the demographics of applicants or the economic conditions in their countries of origin. These factors might change the likelihood of type of visa for which individuals apply. The data are collected for the purpose of providing transparent information regarding the visa operations and to assist in policy making, and research within government agencies as well as for public knowledge. Since FY2019, the methodology for calculating visa data improved for greater accuracy.\n\nData Loading and Cleaning\n\nThe raw data is a 2.3MB excel file downloaded from the travel.state.gov website and stored in the dataset folder. Our load_and_clean_data.R script currently reshapes the data from a 3D country-by-visa-type-by-year format to a 2D country-by-year format where only H1B-type visas are considered. The stored data format is .rds.\nThe original data also includes continent names and totals entered as additional rows before and after every group of countries, which are arranged according to their continent. Our script allocates a continent value to each country based on row order relative to those rows, represented as its own Continent column. The rows containing continent names and totals are then removed. It is also noteworthy that an Unknown continent value exists for the two rows No Nationality and United Nations Laissez-Passer, which denote the two special cases for which US visas may be given without being linked to any specific country’s passport.\n\nData Equity\n\nAcquisition - the dataset comes from the travel state government. It reports statistical information on immigrants and non immigrants visas. Although officials did not explicitly mention the purpose of collecting these visa data annually, it is appropriate to infer that the data serves security and transparency purposes since the travel state government is responsible to provide visa services to foreign nationals who wish to stay in the U.S.\nConception - Because our dataset contains H1B approval from origin countries, it shows immigrant trends and labor market dynamics. For example, governments may analyze H1B approval data to evaluate the effectiveness of immigrant policies attracting talents."
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html",
    "href": "posts/2023-12-20-examples/examples.html",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "href": "posts/2023-12-20-examples/examples.html#figure-sizing",
    "title": "Examples",
    "section": "",
    "text": "Here are some examples of changing the size of a figure.\n\nplot(1:10)\n\n\n\n\n\n\n\n\n\nplot(1:10)\n\n\n\n\nWe can also specify column: screen and out-width: 100% so that the figure will fill the screen. plot in the svg vector graphics file format.\n\nlibrary(ggplot2)\nggplot(pressure, aes(x = temperature, y = pressure)) + geom_point()"
  },
  {
    "objectID": "posts/2024-03-04-blog-post-1/blog-post-1.html",
    "href": "posts/2024-03-04-blog-post-1/blog-post-1.html",
    "title": "Blog Post 1",
    "section": "",
    "text": "The first dataset we looked at is from the US Citizenship and Immigration Services (USCIS) website: https://www.uscis.gov/tools/reports-and-studies/h-1b-employer-data-hub/h-1b-employer-data-hub-files\n\nIt contains 15 csv files ranging from years 2009 to 2023 with about 30,000 rows per csv file and 10 columns including the fiscal year. Looking at the columns of this dataset, it seems it was collected to see how many H1B goes through each year with number of denials, tax id, and the city applicants in. Because this is a tabular dataset, it is possible to load in R easily and start exploration data analysis. One challenge of working with dataset is the fact that it might not help us answer deep questions. For example, if we want to answer questions such as what companies, salary incomes(plotting salary distributions versus other features), ethnicity groups, and gender.\nNote: This same dataset can be found from U.S citizenship immigration services, but someone uploaded to Kaggle.\n\nThe second dataset[s] we will analyze is from travel.state.gov and can be found at: travel.state.gov\n\nThe data includes rows of countries of origin, separated by continent. The columns include Visa type and total visas issued. There is data that includes the month of the issuance as well.\nThis data was published by the Visa Office of the US on the travel.state.gov website. The main question this dataset addresses is the country of origin of visa recipients.\nWe will load the data by using optical character recognition and combine the datasets from each year to form one large dataset. This larger dataset will have multiple rows for each country, one for each year from 2000-2023(maybe month as well) and can be used to evaluate changes in issuances by country over time.\n\nIt may be challenging to load the data because it could be tedious. Also, this data will be difficult to join with other datasets because the most ‘relevant’ joining column will be month/year of issuance, which is not a very specific identifier for this data.\n\nThe first dataset we looked at is from the US Citizenship and Immigration Services (USCIS) website: https://www.uscis.gov/tools/reports-and-studies/h-1b-employer-data-hub/h-1b-employer-data-hub-files\n\nIt contains 15 csv files ranging from years 2009 to 2023 with about 30,000 rows per csv file and 10 columns including the fiscal year. Looking at the columns of this dataset, it seems it was collected to see how many H1B goes through each year with number of denials, tax id, and the city applicants in. Because this is a tabular dataset, it is possible to load in R easily and start exploration data analysis. One challenge of working with dataset is the fact that it might not help us answer deep questions. For example, if we want to answer questions such as what companies, salary incomes(plotting salary distributions versus other features), ethnicity groups, and gender.\nNote: This same dataset can be found from U.S citizenship immigration services, but someone uploaded to Kaggle.\n\nThe second dataset[s] we will analyze is from travel.state.gov and can be found at: travel.state.gov\n\nThe data includes rows of countries of origin, separated by continent. The columns include Visa type and total visas issued. There is data that includes the month of the issuance as well.\nThis data was published by the Visa Office of the US on the travel.state.gov website. The main question this dataset addresses is the country of origin of visa recipients.\nWe will load the data by using optical character recognition and combine the datasets from each year to form one large dataset. This larger dataset will have multiple rows for each country, one for each year from 2000-2023(maybe month as well) and can be used to evaluate changes in issuances by country over time.\nIt may be challenging to load the data because it could be tedious. Also, this data will be difficult to join with other datasets because the most ‘relevant’ joining column will be month/year of issuance, which is not a very specific identifier for this data.\n\nThe last dataset we plan to use is from the US Department of Labor: https://www.dol.gov/agencies/eta/foreign-labor/performance\n\nIt contains detailed performance data on H-1B applications from 2008 to 2023, with approximately 600,000 to 630,000 rows per CSV file and 26 columns. Each row represents one H-1B applicant with their case number. The 26 columns include information such as applicants’ job titles, application dates, wage levels, postal codes, etc. This data was published by the US Department of Labor. We will load the data in R, as all the data are saved in CSV files and organized by year. This dataset contains very precise information, and we want to focus on the applicants’ job titles, wage levels, and their postal codes to find out the economic impact of H-1B visas. One possible challenge I notice is that the dataset contains a huge amount of data; it’s important for us to filter out irrelevant information.\n\n\n\n\n\n\n\n8d31b065024cb4260dfb626578ba4a385d579013"
  },
  {
    "objectID": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "href": "posts/2023-10-13-first-team-meeting/first-team-meeting.html",
    "title": "First Team Meeting",
    "section": "",
    "text": "These are the steps that you will take today to get started on your project. Today, you will just be brainstorming, and then next week, you’ll get started on the main aspects of the project.\n\nStart by introducing yourselves to each other. I also recommend creating a private channel on Microsoft Teams with all your team members. This will be a place that you can communicate and share ideas, code, problems, etc.\nDiscuss what aspects of the project each of you are more or less excited about. These include\n\nCollecting, cleaning, and munging data ,\nStatistical Modeling,\nVisualization,\nWriting about analyses, and\nManaging and reviewing team work.\n\nBased on this, discuss where you feel your strengths and weaknesses might be.\nNext, start brainstorming questions you hope to answer as part of this project. This question should in some way be addressing issues around racial disparities. The questions you come up with should be at the level of the question we started with when exploring the HMDA data. (“Are there differences in the ease of securing a loan based on the race of the applicant?”) You’ll revise your questions a lot over the course of the project. Come up with a few questions that your group might be interested in exploring.\nBased on these questions, start looking around for data that might help you analyze this. If you are looking at U.S. based data, data.gov is a good source and if you are looking internationally, I recommend checking out the World Bank. Also, try Googling for data. Include “data set” or “dataset” in your query. You might even include “CSV” or some other format. Using “data” by itself in your query often doesn’t work too well. Spend some time searching for data and try to come up with at least three possible data sets. (For your first blog post, you’ll write short proposals about each of them that I’ll give feedback on.)\nCome up with a team name. Next week, I’ll provide the Github Classroom assignment that will be where you work on your final project and you’ll have to have your team name finalized by then. Your project will be hosted online at the website with a URL like sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME.\n\nNext time, you’ll get your final project website set up and write your first blog post."
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html",
    "title": "blog-post-4",
    "section": "",
    "text": "readxl"
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html#packages-used",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html#packages-used",
    "title": "blog-post-4",
    "section": "",
    "text": "readxl"
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html#continuation-work",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html#continuation-work",
    "title": "blog-post-4",
    "section": "Continuation work",
    "text": "Continuation work\nIn blog posts 2 and 3, we uncovered a noticeable trend in our second dataset. We analyzed the H1B visa approval rates across different countries by extracting data from an Excel file. One key finding was that Asian countries have the highest H1B approval counts compared to other regions. This discovery led us to delve deeper into understanding the reasons behind this trend and to explore the interconnections with other variables.\nHowever, we faced a challenge with the second dataset as it limited our ability to gain deeper insights or construct statistical models. To overcome this and enrich our analysis, we decided to integrate additional datasets from blog post 1. This integration aims to provide a more comprehensive view and enable a better exploration of the underlying factors influencing the H1B approval trends."
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html#dataset-1",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html#dataset-1",
    "title": "blog-post-4",
    "section": "Dataset 1",
    "text": "Dataset 1\nFor our first dataset, we have the following columns\nFiscal Year: The fiscal year of the H-1B application.\nEmployer: The name of the employer applying for the H-1B visa.\nInitial Approval: The number of initial H-1B applications approved.\nInitial Denial: The number of initial H-1B applications denied.\nContinuing Approval: The number of continuing H-1B applications approved.\nContinuing Denial: The number of continuing H-1B applications denied.\nNAICS: The North American Industry Classification System code.\nTax ID: The Tax Identification Number of the employer.\nState: The state where the employer is located.\nCity: The city where the employer is located.\nZIP: The ZIP code where the employer is located\nWith this dataset, it seems like we can calculate approval and denial rates by employer. We also wonder of the possibility to predict approval/denial with logistic regression. However, we think that the features/columns in this dataset do not represent importances to help classify approval or denial."
  },
  {
    "objectID": "posts/2024-04-07-blogpost4/blogpost4.html#dataset-3",
    "href": "posts/2024-04-07-blogpost4/blogpost4.html#dataset-3",
    "title": "blog-post-4",
    "section": "Dataset 3",
    "text": "Dataset 3\nWith this dataset, it seems like we have a lot of features to explore. However, we can filter columns that contribute to the predicator variable we care about. Specifically, we will explore the most recent year dataset if we want to build statistical model on it. It does not make sense to build a statistical model on year 2010 and use it to explain current year since data distribution is not the same. For this blog post, we decide to start on the year 2024 first to do our exploration and initial modeling. Note that modeling is an iterative process.\n\nlibrary(readxl)\nlibrary(readr)\n\n\nInitial thought\nLooking at this dataset, we want to filter out columns and explore relationships in dataset. Because the original dataset exceeds the limit for Github store without Git LFS, we will saved the cleaned and fewer columns dataset.\nColumns to consider\ncase_number\ncase_status\nreceived_date/decision_date\nVisa-class: Focus only H1B\nJob_title\nfulltime_position\nbegin_date/end_date\nnew_employment\ncontinued_employment\nemployer name: company name\nemployer state\nAgent_representing_employer\njoin with additional dataset with more columns:\nprevailing wage and name of study/colleges\nWe decide to look into these columns since we think that these are high feature importances in terms of making predictive model. Also, we have a hypothesis that a good agent(lawyer) can also increase acceptance of H1B.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ndf &lt;- readRDS(\"dataset/cleaned_disclosure.rds\")\nprint(df)\n\n# A tibble: 101,179 × 12\n   CASE_NUMBER    CASE_STATUS RECEIVED_DATE       DECISION_DATE       VISA_CLASS\n   &lt;chr&gt;          &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;     \n 1 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 2 I-203-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 E-3 Austr…\n 3 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 4 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 5 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 6 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 7 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 8 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n 9 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n10 I-200-23355-5… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n# ℹ 101,169 more rows\n# ℹ 7 more variables: JOB_TITLE &lt;chr&gt;, FULL_TIME_POSITION &lt;chr&gt;,\n#   BEGIN_DATE &lt;dttm&gt;, END_DATE &lt;dttm&gt;, EMPLOYER_NAME &lt;chr&gt;,\n#   EMPLOYER_STATE &lt;chr&gt;, AGENT_REPRESENTING_EMPLOYER &lt;chr&gt;\n\n\n\nnew_df &lt;- df |&gt;\n  filter(\n    VISA_CLASS %in% c(\n      \"H-1B\"\n    )\n  )\n\n\nnew_df$FULL_TIME_POSITION &lt;- factor(new_df$FULL_TIME_POSITION)\n\nnew_df$AGENT_REPRESENTING_EMPLOYER &lt;- factor(new_df$AGENT_REPRESENTING_EMPLOYER)\nmylogit &lt;- glm(FULL_TIME_POSITION ~ AGENT_REPRESENTING_EMPLOYER, data = new_df, family = \"binomial\")\n\n\nsummary(mylogit)\n\n\nCall:\nglm(formula = FULL_TIME_POSITION ~ AGENT_REPRESENTING_EMPLOYER, \n    family = \"binomial\", data = new_df)\n\nCoefficients:\n                               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                     4.27569    0.05054  84.606  &lt; 2e-16 ***\nAGENT_REPRESENTING_EMPLOYERYes -0.44945    0.05702  -7.883  3.2e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 18444  on 97692  degrees of freedom\nResidual deviance: 18377  on 97691  degrees of freedom\nAIC: 18381\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n# model 2\nnew_df$FULL_TIME_POSITION &lt;- factor(new_df$FULL_TIME_POSITION)\n\nnew_df$AGENT_REPRESENTING_EMPLOYER &lt;- factor(new_df$AGENT_REPRESENTING_EMPLOYER)\nnew_df$EMPLOYER_STATE &lt;- factor(new_df$EMPLOYER_STATE)\nmylogit &lt;- glm(FULL_TIME_POSITION ~ AGENT_REPRESENTING_EMPLOYER+EMPLOYER_STATE, data = new_df, family = \"binomial\")\n\n\nsummary(mylogit)\n\n\nCall:\nglm(formula = FULL_TIME_POSITION ~ AGENT_REPRESENTING_EMPLOYER + \n    EMPLOYER_STATE, family = \"binomial\", data = new_df)\n\nCoefficients:\n                                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                       4.14039    0.71619   5.781 7.42e-09 ***\nAGENT_REPRESENTING_EMPLOYERYes   -0.19105    0.06156  -3.103  0.00191 ** \nEMPLOYER_STATEAL                 -1.85110    0.74385  -2.489  0.01283 *  \nEMPLOYER_STATEAR                  0.53670    0.79745   0.673  0.50094    \nEMPLOYER_STATEAZ                  0.13182    0.76300   0.173  0.86284    \nEMPLOYER_STATECA                  0.02224    0.71596   0.031  0.97522    \nEMPLOYER_STATECO                 -0.20357    0.76701  -0.265  0.79069    \nEMPLOYER_STATECT                 -0.28111    0.75499  -0.372  0.70965    \nEMPLOYER_STATEDC                  0.13100    0.84432   0.155  0.87670    \nEMPLOYER_STATEDE                 -0.30400    0.77640  -0.392  0.69539    \nEMPLOYER_STATEFL                 -0.57434    0.72425  -0.793  0.42777    \nEMPLOYER_STATEFM                 11.42568 1029.12171   0.011  0.99114    \nEMPLOYER_STATEGA                 -0.66283    0.72117  -0.919  0.35805    \nEMPLOYER_STATEGU                 11.53518  121.21811   0.095  0.92419    \nEMPLOYER_STATEHI                 -0.57196    1.24162  -0.461  0.64505    \nEMPLOYER_STATEIA                 -0.78786    0.76794  -1.026  0.30492    \nEMPLOYER_STATEID                 -0.52818    0.92393  -0.572  0.56755    \nEMPLOYER_STATEIL                  0.24894    0.72454   0.344  0.73116    \nEMPLOYER_STATEIN                 -0.09901    0.77619  -0.128  0.89850    \nEMPLOYER_STATEKS                 -0.95367    0.77264  -1.234  0.21709    \nEMPLOYER_STATEKY                  0.38863    0.92048   0.422  0.67288    \nEMPLOYER_STATELA                 -0.83444    0.79088  -1.055  0.29139    \nEMPLOYER_STATEMA                 -0.78057    0.71859  -1.086  0.27737    \nEMPLOYER_STATEMD                  0.62599    0.74275   0.843  0.39934    \nEMPLOYER_STATEME                 -0.81796    0.87756  -0.932  0.35129    \nEMPLOYER_STATEMI                 -0.33243    0.72313  -0.460  0.64572    \nEMPLOYER_STATEMN                 -0.30756    0.73904  -0.416  0.67729    \nEMPLOYER_STATEMO                  0.53091    0.77557   0.685  0.49363    \nEMPLOYER_STATEMP                 -1.12560    1.25132  -0.900  0.36837    \nEMPLOYER_STATEMS                 -0.83378    0.92580  -0.901  0.36780    \nEMPLOYER_STATEMT                 11.60222  239.22940   0.048  0.96132    \nEMPLOYER_STATENC                  1.22779    0.76610   1.603  0.10901    \nEMPLOYER_STATEND                  0.37102    1.23386   0.301  0.76364    \nEMPLOYER_STATENE                  2.78154    1.00639   2.764  0.00571 ** \nEMPLOYER_STATENH                 -0.61786    0.87649  -0.705  0.48086    \nEMPLOYER_STATENJ                  0.54768    0.72064   0.760  0.44726    \nEMPLOYER_STATENM                  0.94555    1.23160   0.768  0.44264    \nEMPLOYER_STATENV                 -0.22182    0.87489  -0.254  0.79985    \nEMPLOYER_STATENY                 -0.81520    0.71667  -1.137  0.25534    \nEMPLOYER_STATEOH                 -0.37167    0.73214  -0.508  0.61170    \nEMPLOYER_STATEOK                  0.37214    0.79875   0.466  0.64128    \nEMPLOYER_STATEOR                 -0.72523    0.78334  -0.926  0.35454    \nEMPLOYER_STATEPA                  0.19855    0.72852   0.273  0.78520    \nEMPLOYER_STATEPR                 11.57417  352.84919   0.033  0.97383    \nEMPLOYER_STATERI                  0.22732    0.84415   0.269  0.78771    \nEMPLOYER_STATESC                 -0.53251    0.79013  -0.674  0.50034    \nEMPLOYER_STATESD                 -2.01863    0.79765  -2.531  0.01138 *  \nEMPLOYER_STATETN                 -0.08546    0.74711  -0.114  0.90893    \nEMPLOYER_STATETX                  0.21922    0.71860   0.305  0.76032    \nEMPLOYER_STATEUT                  0.12953    0.84435   0.153  0.87807    \nEMPLOYER_STATEVA                 -0.24928    0.72214  -0.345  0.72994    \nEMPLOYER_STATEVI                 11.52463  727.28733   0.016  0.98736    \nEMPLOYER_STATEVT                  0.61491    0.87447   0.703  0.48194    \nEMPLOYER_STATEWA                  1.17962    0.73693   1.601  0.10944    \nEMPLOYER_STATEWI                  0.10976    0.77589   0.141  0.88751    \nEMPLOYER_STATEWV                 -2.07776    0.89213  -2.329  0.01986 *  \nEMPLOYER_STATEWY                 -1.83357    0.83402  -2.198  0.02791 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 18443  on 97679  degrees of freedom\nResidual deviance: 17775  on 97623  degrees of freedom\n  (13 observations deleted due to missingness)\nAIC: 17889\n\nNumber of Fisher Scoring iterations: 14\n\n\nFor AGENT_REPRESENTING_EMPLOYERYes, the coefficient is -0.19105, indicating that when the agent is representing the employer, the log-odds of having a full-time position (“Y”) decrease compared to when the agent is not representing (the baseline). This contradicts to our assumption that having a good lawyer increases the chance of H1B approval. Perhaps, it is worth investigate further.\nFor state, MA(Massachusetts) and NY(New York) have a similiar coefficient, that is, negative effect on the likelihood of getting full-time position.\nOverall, we can see that there exist features in dataset that contribute to whether likely to being offer fulltime position or not. If possible, we can join the remaining two datasets with case number which gives us more features to work on."
  },
  {
    "objectID": "posts/2023-10-15-getting-started/getting-started.html",
    "href": "posts/2023-10-15-getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Below, the items marked with [[OP]] should only be done by one person on the team.\n\nTo get started\n\n[[OP]] One person from the team should click the Github Classroom link on Teams.\n[[OP]] That person types in the group name for their group.\nThe rest of the team now clicks the Github Classroom link and selects their team from the dropdown list.\nFinally, each of you can clone the repository to your laptop like a normal assignment.\n\n\n\nSetting up the site\n\n[[OP]] Open the terminal and run quarto publish gh-pages.\n[[OP]] Select Yes to the prompt:  ? Publish site to https://sussmanbu.github.io/ma4615-fa23-final-project-TEAMNAME/ using gh-pages? (Y/n)\n[[OP]] Wait for the process to finish.\nOnce it is done, you can go to the URL it asked you about to see your site.\n\nNote: This is the process you will use every time you want to update your published site. Make sure to always follow the steps below for rendering, previewing, and committing your changes before doing these publish steps. Anyone can publish in the future.\n\n\nCustomize your site\n\n[[OP]] Open the _quarto.yml file and update the title to include your team name.\n[[OP]] Go to the about.qmd and remove the TF’s and professor’s names.\nadd your own along with a short introduction and a link to your Github user page.\n[[OP]] Render the site.\n[[OP]] Check and make sure you didn’t get any errors.\n[[OP]] Commit your changes and push.\n[[OP]] Repeat the steps under Setting up your site.\n\nOnce one person is done with this, each teammate in the group can, in turn, repeat steps 3-7. Before doing so, make sure to pull the changes from teammates before starting to make new changes. (We’ll talk soon about ways to organize your work and resolve conflicts.)\n\n\nStart your first post\n\nTo start your first post first, run remotes::install_github(\"sussmanbu/quartopost\") in your Console.\n[[OP]] Run quartopost::quartopost() (or click Addins-&gt;Create Quarto Post, or use C-Shift-P, type “Create Quarto” and press enter to run the command).\n\nNow you can start working on your post. You’ll want to render your post to see what it will look like on the site.\n\nEvery time you want to make a new post, you can repeat step 2 above.\nWhen you want to publish your progress, follow steps 4-7 from Customize your site.\n\nFinally, make sure to read through everything on this site which has the directions and rubric for the final project."
  },
  {
    "objectID": "posts/2024-02-23-test1/test1.html",
    "href": "posts/2024-02-23-test1/test1.html",
    "title": "test_1",
    "section": "",
    "text": "testing"
  },
  {
    "objectID": "posts/2024-04-10-blogpost5/blogpost5.html",
    "href": "posts/2024-04-10-blogpost5/blogpost5.html",
    "title": "blogpost5",
    "section": "",
    "text": "library(readxl)\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.2.0\n✔ dials        1.2.0     ✔ tibble       3.2.1\n✔ ggplot2      3.4.4     ✔ tidyr        1.3.0\n✔ infer        1.0.5     ✔ tune         1.1.2\n✔ modeldata    1.2.0     ✔ workflows    1.1.3\n✔ parsnip      1.1.1     ✔ workflowsets 1.0.1\n✔ purrr        1.0.2     ✔ yardstick    1.2.0\n✔ recipes      1.0.9     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()  masks scales::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(cluster)\nlibrary(ggplot2)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\ndf &lt;- readRDS(\"dataset/cleaned_disclosure.rds\")\nhead(df)\n\n# A tibble: 6 × 12\n  CASE_NUMBER     CASE_STATUS RECEIVED_DATE       DECISION_DATE       VISA_CLASS\n  &lt;chr&gt;           &lt;chr&gt;       &lt;dttm&gt;              &lt;dttm&gt;              &lt;chr&gt;     \n1 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n2 I-203-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 E-3 Austr…\n3 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n4 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n5 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n6 I-200-23355-58… Certified   2023-12-21 00:00:00 2023-12-29 00:00:00 H-1B      \n# ℹ 7 more variables: JOB_TITLE &lt;chr&gt;, FULL_TIME_POSITION &lt;chr&gt;,\n#   BEGIN_DATE &lt;dttm&gt;, END_DATE &lt;dttm&gt;, EMPLOYER_NAME &lt;chr&gt;,\n#   EMPLOYER_STATE &lt;chr&gt;, AGENT_REPRESENTING_EMPLOYER &lt;chr&gt;"
  },
  {
    "objectID": "posts/2024-04-10-blogpost5/blogpost5.html#merging-process",
    "href": "posts/2024-04-10-blogpost5/blogpost5.html#merging-process",
    "title": "blogpost5",
    "section": "Merging Process",
    "text": "Merging Process\nTo merge the dataset together, we can find matching case numbers.\n\n# merge df1 and df2 by case number\n# merge_1 &lt;- merge(df, df2, by=\"CASE_NUMBER\", all = TRUE)\n\n\n# final_merge &lt;- merge(merge_1, df3, by=\"CASE_NUMBER\", all = TRUE)\n\n\n# visualize our new dataset\n# final_merge\n\n\n# save as new rds for final_merge\n# saveRDS(final_merge, \"dataset/final_merge.rds\")"
  },
  {
    "objectID": "posts/2024-03-23-blogpost3/blogpost3.html",
    "href": "posts/2024-03-23-blogpost3/blogpost3.html",
    "title": "blog-post-3",
    "section": "",
    "text": "Further cleaning: we discovered that one of the rows of the Unknown continent, United Nations Laissez-Passer, contained only empty rows. This suggests that this Country value might have had some nonzero values for some non-H1B visa types in the original dataset, but since it has zero H1B visas given to it over all the years, we deleted this row.\nData exploration: [comment on data exploration here, explaining the following 3 graphs:]\n\n\n# run this if you don't have the magick package installed\n# install.packages(\"magick\")\n# make sure to comment install.package out before creating gh-pages.\n\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\nimage1 &lt;- image_read('images/total_bar_stacked.png')\nplot(image1)\n\n\n\n\nObservation: we see a positive trend on the number of H1B approvals. As the year progresses, the number of H1B approvals continue to rise. However, we notice that there exist a sharp drop in year 2020 and 2021 particularly due to covid. Next, it spikes back up in year 2022. From this plot, we also see that Asia has been the consistent country where H1B approvals happen the most.\n\nimage2 &lt;- image_read('images/total_bar.png')\nplot(image2)\n\n\n\n\nFor this side by side comparison of H1B approvals and other countries, we get to observe the trend of each countries. It seems like Asia has been getting positive trends since year 1997 till 2022 except 2020 and 2021 due to covid. In contrast, other countries like Europe, South America, Oceania and Africa have all been trending downward for H1B approval. It can be worth to understand what factors contribute to the trends.\n\nimage3 &lt;- image_read('images/percent_bar_faceted.png')\nplot(image3)\n\n\n\n\nThe last plot shows similar content to plot 2. However, with percentage being display, we see that Asia out numbered all other countries combined in terms of H1B approval and Europe comes in second place."
  },
  {
    "objectID": "DataExp.html",
    "href": "DataExp.html",
    "title": "MA [46]15 Final Project T3am-3",
    "section": "",
    "text": "&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n\nsuppressPackageStartupMessages(library(tidyverse))\n#filename &lt;- file.choose(\"cleaned_h1b_data.rds\")\n#print(filename)\nh1b_country &lt;- \n  read_rds('dataset/cleaned_h1b_data.rds')\n\n\n#changing the year to numerical values\ncurrent_names &lt;- names(h1b_country)\nprint(current_names)\n\n [1] \"Country\"   \"Continent\" \"FY97\"      \"FY98\"      \"FY99\"      \"FY00\"     \n [7] \"FY01\"      \"FY02\"      \"FY03\"      \"FY04\"      \"FY05\"      \"FY06\"     \n[13] \"FY07\"      \"FY08\"      \"FY09\"      \"FY10\"      \"FY11\"      \"FY12\"     \n[19] \"FY13\"      \"FY14\"      \"FY15\"      \"FY16\"      \"FY17\"      \"FY18\"     \n[25] \"FY19\"      \"FY20\"      \"FY21\"      \"FY22\"     \n\n#names(df) &lt;- new_names\n\n\ncolnames(h1b_country) &lt;- c('Country','Continent','1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022') \n\nh1b_country\n\n# A tibble: 199 × 28\n   Country     Continent `1997` `1998` `1999` `2000` `2001` `2002` `2003` `2004`\n   &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Algeria     Africa        52     49     53     64     75     43     17     36\n 2 Angola      Africa         0      0      2      2      2      4      6      8\n 3 Benin       Africa        10      4      5      8     13      8      7     13\n 4 Botswana    Africa         2      5      5      3      9      8     11     21\n 5 Burkina Fa… Africa         3      6      8      4      6     13     16     14\n 6 Burundi     Africa         1      1      1      1      0      0      1      0\n 7 Cabo Verde  Africa        NA     NA     NA     NA     NA     NA     NA     NA\n 8 Cameroon    Africa        36     50     62     59     80     79     64     68\n 9 Central Af… Africa         3      0      2      1      0      6      2      2\n10 Chad        Africa         0      4      4      1      2      1      3      4\n# ℹ 189 more rows\n# ℹ 18 more variables: `2005` &lt;dbl&gt;, `2006` &lt;dbl&gt;, `2007` &lt;dbl&gt;, `2008` &lt;dbl&gt;,\n#   `2009` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2011` &lt;dbl&gt;, `2012` &lt;dbl&gt;, `2013` &lt;dbl&gt;,\n#   `2014` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2016` &lt;dbl&gt;, `2017` &lt;dbl&gt;, `2018` &lt;dbl&gt;,\n#   `2019` &lt;dbl&gt;, `2020` &lt;dbl&gt;, `2021` &lt;dbl&gt;, `2022` &lt;dbl&gt;\n\n\n\nh1b_continent &lt;- h1b_country |&gt;\n  group_by(Continent)|&gt;\n  select(-Country)|&gt;\n  summarize(across(everything(), sum, na.rm = TRUE))\n\nWarning: There was 1 warning in `summarize()`.\nℹ In argument: `across(everything(), sum, na.rm = TRUE)`.\nℹ In group 1: `Continent = \"Africa\"`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\nlibrary(tidyr)\ndata_long &lt;- pivot_longer(h1b_continent, cols = -Continent, names_to = \"year\", values_to = \"count_h1b\")\n\ndata_long\n\n# A tibble: 182 × 3\n   Continent year  count_h1b\n   &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1 Africa    1997       2518\n 2 Africa    1998       2634\n 3 Africa    1999       2835\n 4 Africa    2000       3126\n 5 Africa    2001       3808\n 6 Africa    2002       2903\n 7 Africa    2003       2456\n 8 Africa    2004       3022\n 9 Africa    2005       2794\n10 Africa    2006       2886\n# ℹ 172 more rows\n\n\n\nggplot(data_long, aes(x = year, y = count_h1b, fill = Continent)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  labs(title = \"Total Values by Continent Over the Years\",\n       x = \"Year\", y = \"Total Value\") + theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nggplot(data_long, aes(x = year, y = count_h1b, fill = Continent)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  facet_wrap(~Continent, scales='free_y') + \n  labs(title = \"Total Values by Continent Over the Years\",\n       x = \"Year\", y = \"Total Value\") + theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1)) +\n  scale_x_discrete(guide=guide_axis(check.overlap=TRUE))\n\n\n\n\n\nh1b_continent &lt;- h1b_country |&gt;\n  filter(Continent == 'Unknown') |&gt;\n  select(-Continent)\n\n# library(tidyr)\nunknown_longer &lt;- pivot_longer(h1b_continent, cols = -Country, names_to = \"year\", values_to = \"count_h1b\")\n# \nunknown_longer\n\n# A tibble: 26 × 3\n   Country        year  count_h1b\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;\n 1 No Nationality 1997        209\n 2 No Nationality 1998         97\n 3 No Nationality 1999         67\n 4 No Nationality 2000         56\n 5 No Nationality 2001         52\n 6 No Nationality 2002         15\n 7 No Nationality 2003          7\n 8 No Nationality 2004         15\n 9 No Nationality 2005         13\n10 No Nationality 2006         14\n# ℹ 16 more rows\n\n\n\nnames(unknown_longer)\n\n[1] \"Country\"   \"year\"      \"count_h1b\"\n\n\n\nclean_un &lt;- unknown_longer |&gt;\n  filter(Country == 'United Nations Laissez-Passer')|&gt;\n  filter(count_h1b != 0)\n\nclean_un\n\n# A tibble: 0 × 3\n# ℹ 3 variables: Country &lt;chr&gt;, year &lt;chr&gt;, count_h1b &lt;dbl&gt;\n\n\nThis shows that no one with a nationality designated as ‘United Nations Laissez-Passer’ has received an h1b visa from the years 1997-2022, so we can clear that.\n\nggplot(unknown_longer, aes(x = year, y = count_h1b, fill = Country)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  #facet_wrap(~Continent, scales='free_y')+\n  labs(title = \"Total Values by Continent Over the Years\",\n       x = \"Year\", y = \"Total Value\") + theme(legend.position = \"top\",\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(ggplot2)  # for ggplot (optional if you want to use ggplot for visualization)\n\n# Sample data manipulation assuming h1b_country is your dataset\nh1b_country_temp &lt;- h1b_country %&gt;%\n  select(-Continent)\n\n# Convert the data from wide to long format\ndata_long_all &lt;- pivot_longer(h1b_country_temp, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\n# Filter for only recent years 2022, 2021, 2020, etc.\nyears_of_interest &lt;- c(\"2022\", \"2021\", \"2020\", \"2019\", \"2018\")\ndata_filtered &lt;- data_long_all %&gt;%\n  filter(year %in% years_of_interest)\n\n# Initialize an empty list to store the pie charts\npiecharts_all &lt;- list()\n\n# Loop only over the filtered years\nfor (year_iter in years_of_interest) {\n  print(year_iter)\n  all_temp &lt;- data_filtered %&gt;% \n    filter(year == year_iter)\n  \n  # Select the top 5 countries based on the count\n  top_countries &lt;- all_temp %&gt;%\n    group_by(Country) %&gt;%\n    summarise(total_count = sum(count_h1b), .groups = \"drop\") %&gt;%\n    arrange(desc(total_count)) %&gt;%\n    slice_max(order_by = total_count, n = 5)  # Updated to use slice_max which is more explicit\n  \n  # Summarise the rest as 'Other'\n  other_countries &lt;- all_temp %&gt;%\n    anti_join(top_countries, by = \"Country\") %&gt;%\n    filter(!is.na(count_h1b)) %&gt;%\n    summarise(Country = \"Other\",\n              total_count = sum(count_h1b), .groups = \"drop\")\n  \n  # Combine the top countries and 'Other'\n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  # Create pie chart\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  # Store the pie chart in the list\n  piecharts_all[[year_iter]] &lt;- pie_chart\n}\n\n[1] \"2022\"\n\n\n\n\n\n[1] \"2021\"\n\n\n\n\n\n[1] \"2020\"\n\n\n\n\n\n[1] \"2019\"\n\n\n\n\n\n[1] \"2018\"\n\n\n\n\n# Print all pie charts stored in the list\nprint(piecharts_all)\n\nlist()\n\n\n\nh1b_africa &lt;- h1b_country |&gt;\n  filter(Continent == 'Africa') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_africa &lt;- pivot_longer(h1b_africa, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\n\n  africa_temp &lt;- data_long_africa |&gt; filter(year == 1997)\n  \n  #choose top 5\n  top_countries &lt;- africa_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n\nSelecting by total_count\n\n  #summarise the rest\n  other_countries &lt;- africa_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n  filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",total_count = sum(count_h1b))\n  \n  other_countries\n\n# A tibble: 1 × 2\n  Country total_count\n  &lt;chr&gt;         &lt;dbl&gt;\n1 Other           513\n\n\n\npiecharts &lt;- list()\nfor(year_iter in unique(data_long_africa$year)){\n  africa_temp &lt;- data_long_africa |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- africa_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- africa_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 African Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts[[year_iter]] &lt;- pie_chart\n  \n}\n\nSelecting by total_count\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts)\n\nlist()\n\n\n\n#do the same for other continents\n\nh1b_asia &lt;- h1b_country |&gt;\n  filter(Continent == 'Asia') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_asia &lt;- pivot_longer(h1b_asia, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_asia &lt;- list()\n\nfor(year_iter in unique(data_long_asia$year)){\n  print(year_iter)\n  asia_temp &lt;- data_long_asia |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- asia_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- asia_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 Asian Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_asia[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_asia)\n\nlist()\n\n\n\n#do the same for other continents\nh1b_europe &lt;- h1b_country |&gt;\n  filter(Continent == 'Europe') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_europe &lt;- pivot_longer(h1b_europe, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_europe &lt;- list()\n\nfor(year_iter in unique(data_long_europe$year)){\n  print(year_iter)\n  europe_temp &lt;- data_long_europe |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- europe_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- europe_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 European Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_europe[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_europe)\n\nlist()\n\n\n\nh1b_namerica &lt;- h1b_country |&gt;\n  filter(Continent == 'North America') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_namerica &lt;- pivot_longer(h1b_namerica, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_namerica &lt;- list()\n\nfor(year_iter in unique(data_long_namerica$year)){\n  print(year_iter)\n  namerica_temp &lt;- data_long_namerica |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- namerica_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- namerica_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 North American Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_namerica[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_namerica)\n\nlist()\n\n\n\nh1b_samerica &lt;- h1b_country |&gt;\n  filter(Continent == 'South America') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_samerica &lt;- pivot_longer(h1b_samerica, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_samerica &lt;- list()\n\nfor(year_iter in unique(data_long_samerica$year)){\n  print(year_iter)\n  samerica_temp &lt;- data_long_samerica |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- samerica_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(5)\n  \n  #summarise the rest\n  other_countries &lt;- samerica_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 South American Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_samerica[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_samerica)\n\nlist()\n\n\n\nh1b_oceania &lt;- h1b_country |&gt;\n  filter(Continent == 'Oceania') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_oceania &lt;- pivot_longer(h1b_oceania, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\npiecharts_oceania &lt;- list()\n\nfor(year_iter in unique(data_long_oceania$year)){\n  print(year_iter)\n  oceania_temp &lt;- data_long_oceania |&gt; filter(year == year_iter)\n  \n  #choose top 5\n  top_countries &lt;- oceania_temp |&gt;\n  group_by(Country) |&gt;\n  summarise(total_count = count_h1b) |&gt;\n  arrange(desc(total_count)) |&gt;\n  top_n(3)\n  \n  #summarise the rest\n  other_countries &lt;- oceania_temp |&gt;\n  anti_join(top_countries, by = \"Country\") |&gt;\n    filter(!is.na(count_h1b))|&gt;\n  summarise(Country = \"Other\",\n            total_count = sum(count_h1b))\n  \n  combined_data &lt;- bind_rows(top_countries, other_countries)\n  pie_chart &lt;- pie(combined_data$total_count, labels = combined_data$Country, main = paste(\"Top 5 Oceanian Countries in\", year_iter), col=viridis(length(combined_data$total_count)))\n  \n  piecharts_oceania[[year_iter]] &lt;- pie_chart\n  \n}\n\n[1] \"1997\"\n\n\nSelecting by total_count\n\n\n[1] \"1998\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"1999\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2000\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2001\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2002\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2003\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2004\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2005\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2006\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2007\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2008\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2009\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2010\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2011\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2012\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2013\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2014\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2015\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2016\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2017\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2018\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2019\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2020\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2021\"\n\n\nSelecting by total_count\n\n\n\n\n\n[1] \"2022\"\n\n\nSelecting by total_count\n\n\n\n\n\n\n\nprint(piecharts_oceania)\n\nlist()\n\n\n\nh1b_other &lt;- h1b_country |&gt;\n  filter(Continent == 'Unknown') |&gt;\n  select(-Continent)\n  #summarize(across(where(is.numeric), sum, na.rm = TRUE))\n\ndata_long_other &lt;- pivot_longer(h1b_other, cols=-Country, names_to = \"year\", values_to = \"count_h1b\")\n\ndata_long_other\n\n# A tibble: 26 × 3\n   Country        year  count_h1b\n   &lt;chr&gt;          &lt;chr&gt;     &lt;dbl&gt;\n 1 No Nationality 1997        209\n 2 No Nationality 1998         97\n 3 No Nationality 1999         67\n 4 No Nationality 2000         56\n 5 No Nationality 2001         52\n 6 No Nationality 2002         15\n 7 No Nationality 2003          7\n 8 No Nationality 2004         15\n 9 No Nationality 2005         13\n10 No Nationality 2006         14\n# ℹ 16 more rows\n\n\n=======\n\n# Load libraries and dataset\nsuppressPackageStartupMessages(library(tidyverse))\nh1b_country &lt;- read_rds('dataset/cleaned_h1b_data.rds')\n\n\n# Rename year columns\nyears &lt;- paste0(1997:2022)\ncolnames(h1b_country) &lt;- c('Country', 'Continent', years)\n\n\n# Summarize total H1B visas per year and transform into long format for plotting\ntotal_h1b_by_year_long &lt;- h1b_country %&gt;%\n  select(all_of(years)) %&gt;%\n  summarise(across(everything(), sum, na.rm = TRUE)) %&gt;%\n  pivot_longer(cols = everything(), names_to = \"year\", values_to = \"Total_H1B_Visas\")\n\n# Plot total H1B visas issued worldwide over the years\nggplot(total_h1b_by_year_long, aes(x = year, y = Total_H1B_Visas, fill = year)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Total H1B Visas Issued Worldwide Over the Years\",\n       x = \"Year\", y = \"Total Number of H1B Visas\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1), legend.position = \"none\")\n\n\n\n\n\n# Summarize data by continent and transform into long format\ndata_long &lt;- h1b_country %&gt;%\n  group_by(Continent) %&gt;%\n  summarise(across(all_of(years), sum, na.rm = TRUE)) %&gt;%\n  pivot_longer(cols = all_of(years), names_to = \"year\", values_to = \"count_h1b\")\n\n# Plot total values by continent over the years\nggplot(data_long, aes(x = year, y = count_h1b, fill = Continent)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~Continent, scales=\"free_y\") +\n  scale_x_discrete(guide = guide_axis(check.overlap = TRUE)) +\n  labs(title = \"Total H1B Visas by Continent Over the Years\",\n       x = \"Year\", y = \"Total Visas\") +\n  theme(legend.position = \"top\", axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.margin = unit(c(1, 1, 1, 1), \"cm\"))\n\n\n\n\n\n\n\n\n\n\n\n5dcbcb76829b43c5323c8d54ece3b8995ca08211"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This comes from the file about.qmd.\nThis is a website for the final project for MA[46]15 Data Science with R by Team TEAMNAME. The members of this team are below."
  },
  {
    "objectID": "about.html#wei",
    "href": "about.html#wei",
    "title": "About",
    "section": "Wei",
    "text": "Wei\nSenior studying Data Science"
  },
  {
    "objectID": "about.html#xinran-sun",
    "href": "about.html#xinran-sun",
    "title": "About",
    "section": "Xinran Sun",
    "text": "Xinran Sun\nXinran is a junior majoring in Economics and Mathematics at Boston University."
  },
  {
    "objectID": "about.html#ellis-coldren",
    "href": "about.html#ellis-coldren",
    "title": "About",
    "section": "Ellis Coldren",
    "text": "Ellis Coldren\nSenior studying math and computer science in the BA/MA program."
  },
  {
    "objectID": "about.html#phil-ledoit",
    "href": "about.html#phil-ledoit",
    "title": "About",
    "section": "Phil Ledoit",
    "text": "Phil Ledoit\nPhil is a senior studying Statistics and Computer Science. His interests include data exploration, cleaning, and processing."
  },
  {
    "objectID": "about.html#ken-yusuf",
    "href": "about.html#ken-yusuf",
    "title": "About",
    "section": "Ken Yusuf",
    "text": "Ken Yusuf\nSophomore studying economics and math.\n\n\nAbout this Template.\nThis is based off of the standard Quarto website template from RStudio (2023.09.0 Build 463)."
  },
  {
    "objectID": "big_picture.html",
    "href": "big_picture.html",
    "title": "Big Picture",
    "section": "",
    "text": "This comes from the file big_picture.Rmd.\nThink of this page as your 538/Upshot style article. This means that you should try to tell a story through the data and your analysis. Read articles from those sites and similar sites to get a feeling for what they are like. Try to write in the style of a news or popular article. Importantly, this pge should be geared towards the general public. You shouldn’t assume the reader understands how to interpret a linear regression. Focus on interpretation and visualizations."
  },
  {
    "objectID": "big_picture.html#rubric-on-this-page",
    "href": "big_picture.html#rubric-on-this-page",
    "title": "Big Picture",
    "section": "Rubric: On this page",
    "text": "Rubric: On this page\nYou will\n\nTitle\n\nYour big picture page should have a creative/click-bait-y title/headline that provides a hint about your thesis.\n\nClarity of Explanation\n\nYou should have a clear thesis/goal for this page. What are you trying to show? Make sure that you explain your analysis in detail but don’t go into top much mathematics or statistics. The audience for this page is the general public (to the extent possible). Your thesis should be a statement, not a question.\nEach figure should be very polished and also not too complicated. There should be a clear interpretation of the figure so the figure has a clear purpose. Even something like a histogram can be difficult to interpret for non-experts.\n\nCreativity\n\nDo your best to make things interesting. Think of a story. Think of how each part of your analysis supports the previous part or provides a different perspective.\n\nThis page should be self-contained.\n\nNote: This page should have no code visible, i.e. use #| echo: FALSE."
  },
  {
    "objectID": "big_picture.html#rubric-other-components",
    "href": "big_picture.html#rubric-other-components",
    "title": "Big Picture",
    "section": "Rubric: Other components",
    "text": "Rubric: Other components\n\nInteractive\nYou will also be required to make an interactive dashboard like this one.\nYour Big Data page should include a link to an interactive dashboard. The dashboard should be created either using Shiny or FlexDashboard (or another tool with professor’s approval). This interactive component should in some way support your thesis from your big picture page. Good interactives often provide both high-level understanding of the data while allowing a user to investigate specific scenarios, observations, subgroups, etc.\n\nQuality and ease of use of the interactive components. Is it clear what can be explored using your interactive components? Does it enhance and reinforce your conclusions from the Big Picture? Plotly with default hover text will get no credit. Be creative!\n\n\n\nVideo Recording\nMake a video recording (probably using Zoom) demonstrating your interactive components. You should provide a quick explanation of your data and demonstrate some of the conclusions from your EDA. This video should be no longer than 4 minutes. Include a link to your video (and password if needed) in your README.md file on your Github repository. You are not required to provide a link on the website. This can be presented by any subset of the team members.\n\n\nRest of the Site\nFinally, here are important things to keep in mind for the rest of the site.\nThe main title of your page is informative. Each post has an author/description/informative title. All lab required posts are present. Each page (including the home page) has a nice featured image associated with it. Your about page is up to date and clean. You have removed the generic posts from the initial site template."
  }
]